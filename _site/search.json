[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/05-hypothesis_testing/index.html",
    "href": "posts/05-hypothesis_testing/index.html",
    "title": "Project 5: Hypothesis Testing",
    "section": "",
    "text": "1.\nI think the author means that people will focus less on p &lt; .05 and the significance of their results and instead focus on actually analyzing and discussing their results. De-emphasizing the p-value and statistical significance would allow for more discussion of less than perfect results. Instead of fixating on results that fit within the constraints of p &lt; .05, researchers could discuss results that may have higher p-values. Doing so would allow for more analysis into what worked and what didn’t and how such results may have occurred, thus improving overall knowledge and potentially even improving the test or experiment used. I also think that getting rid of the use of statistical significance would result in researchers having to consider other means of statistical analysis and inference, forcing them to question the effectiveness and usefulness of different statistical tools, compare models, and analyze their assumptions about the data and results, which is statistical thinking. Instead of basing their explanations primarily around significance, they may analyze their data to see what is actually occurring rather than what is “significant”. I think that to embody statistical thinking, it is required to consider any potential biases and view the problem from a neutral standpoint. For example, it may be a popular sentiment that package theft has been on the rise due to so many neighbors having their packages stolen from their porch. However, if you were to take data on porch piracy and analyze it via graphing and modeling, you would see that porch piracy has actually been decreasing, and that the belief of an increase may come from exposure bias. Statistical thinking isn’t just about acknowledging bias and conducting analyses to disprove common beliefs, however. It’s also about applying statistical tools and techniques to solve problems. For example, politicians and statisticians may work together to decrease the amount of opiate overdoses by using statistics from previous years about emergency response times, education, and the prevalence of narcan use in responding to opiate overdoses. Statisticians could use different tools such as modeling that may result in suggestions of an increase in narcan supply, the necessity of faster emergency response times, and an increase in education. Politicians could then use that information to implement different policies to do such a thing. However, results that may occur from such modeling are not foolproof, and in order to completely embody statistical thinking, it’s necessary to acknowledge the potential shortcomings and limits of models and analyses.\n\n\n2.\nThe author means that treating p-values as a means of determining what is significant and what is insignificant is pointless because this idea of significance comes from a misunderstanding of its original use. Significance is often associated with usefulness, importance, and probability while insignificance is associated with unimportance and improbability. The intention of the use of the word significance was to indicate whether or not more scrutiny was needed - it was never meant to be used in the way that it is used today. Now, p-values are used as a part of determining the rejection of the null hypothesis in hypothesis testing where questions of significance are regularly brought up and the rejection or failure or rejection involves explanations in determining whether or not something was statistically significant. When the author mentions the dichotomization of p-values, they are talking about the categorization of “significant” and “not significant.” The issue with the dichotomization of the p-value is that it incorrectly establishes an authority of the p-value where it can be used to determine the significance or lack thereof in a way where significance is misunderstood as providing proof of plausibility and importance of a result. It contributes to black and white views of results, where one side is deemed worthy of consideration and the other is deemed unworthy and disregarded. This can result in researchers prioritizing certain results when the disregarded results may have provided interesting insight into their research. It creates this idea that significance should be valued above all when there are many other useful statistical inferences that have been disregarded alongside unworthy results.\n\n\n3.\nI agree that p-values shouldn’t have to pass a threshold in order to choose which results to present or highlight because as the paper points out, it could result in bias since the results that fit the proper threshold will get published and reported, leaving out other results that may be of interest. That interest may not be due to “correctness” (i.e. fitting a desired outcome) or what works best, but rather something that can be learned. I think that it’s useful to highlight results that have occurred due to an error or a deviation from the intended experiment or process, as it can help researchers understand what’s happening, how, and what aspect impacts or causes certain results. With that being said, I don’t think that there’s a one size fits all solution to determining what gets presented in publishing, rather it’s a case by case basis. I believe that results that reveal information about the data, regardless of whether they were the desired outcome or the product of a mistake, would serve as noteworthy inclusions given that they help researchers further understand a topic or process. When it comes to “bad” results, it’s always good to look back on what went wrong and how to adjust the study or experiment in the future. I think that reproducibility is an important aspect as well, as it allows others to see where results have come from and further their understanding as to why and what’s been impacted. I think that there are some cases, such as the FDA’s, where p-values can be used to determine what gets published, if only until a better solution for interpreting the results is found since some organizations and researchers have become very reliant on the p-values.\n\n\n4.\nI agree that there’s no one size fits all approach to statistical inference in scientific research because there are too many fields of science and too many types of research to come up with one approach that works well for all of them. Each field has its own standards, with some standards being more similar than others. There is bound to be some conflict in what is appropriate and what is inappropriate in certain fields. I believe that what works best is dependent on the specific study or experiment being done, as some statistical tools and methods might be of better use than others. The FDA example given further in the reading is a good example of this. While it may be possible for them to move away from their reliance on p-values, it’s not an easy task considering that they have been doing it for such a long time and that it’s worked for them. It’s much easier to establish general principles like ATOM and allow the researcher to use what works best for their research, as long as they provide extensive justifications for what they are choosing to do and why.\n\n\n5.\nI think that statistical thoughtfulness means putting everything into consideration. It’s considering all of the prior information and context of the data you are looking at, potential results of the study/experiment, careful designs of your study/experiment, and the utilization of various methods of analysis when looking at results. As said in the text, it’s also about “investing in producing solid data” (page 3, section 3).To demonstrate statistical thoughtfulness, researchers should ask questions pertaining to their data. For example, they should be considering whether or not they made the correct assumptions when analyzing their data, question the precision of their estimates, and whether or not the study was adequately designed. Statistical thoughtfulness to me also means being very thorough in your analysis and study, considering possible outcomes and being able to justify what you’re doing and why. It also involves putting thought into how results and data are conveyed, and their implications. Another way to demonstrate statistical thoughtfulness is by making sure that your data and results are explained in ways that are accessible to your audience. An example of this is seen in 3.2.4, where some authors suggest replacing the words significant and confidence with compatibility so that the terms aren’t misunderstood. Overall, I consider statistical thoughtfulness to be very similar, if not nearly identical to statistical thinking, with statistical thoughtfulness being more thorough and purposeful in what is being done and more research related, while statistical thinking is better suited for non-resarch occurrences and applications.\n\n\n6.\nI think that the authors believe that the problem is that people see words like significance and confidence and make the wrong associations. People associate significance with importance and necessity and confidence with certainty, so when they are looking at confidence intervals, they’re seeing an interval of “correct” data with the incorrect and useless values left out of the interval and to be considered no longer. In combination with the prioritization of a small p-value due to a misconstrued understanding of significance, the author believes that there’s a risk of overconfidence which I believe may stem from the acceptance of confidence intervals that are too narrow simply because it contains a certain value. I think the overconfidence is not necessarily about the narrowness of the interval, but also in regards to misunderstood interpretations of confidence intervals, where the intervals are treated less as estimates and more as significant values. The authors introduced the term compatibility to try and rid p-values and confidence intervals of misunderstood associations and to encourage people to view confidence intervals as producing estimates that are compatible with the data under the created model instead of values that the data should fall into. It’s a similar situation with p-values, where compatibility can be used to shape the view of p-values as “measuring compatibility between hypothesis and data” (section 3.2.4) instead of p-values as determining significance. I think that the overall goal is to view confidence intervals and p-values in relation to the data instead of as a descriptor of the data.\nI agree that there may be an issue with people misunderstanding the words significance and confidence, resulting in the misunderstanding of confidence intervals and p-values. However, I don’t really agree that the solution is to change the wording because I feel like compatibility may still be misunderstood in a similar way to the words significance and confidence, thus resulting in the similar treatment. I think the better course of action is to provide more in depth explanations about what is actually happening and what the p-values and confidence intervals are actually telling you and what they actually represent. Changing the words to compatibility may help with some context, but I think in order to get people to fully understand what is going on, you need to explain it to them instead of relying on implicit understanding.\n\n\n7.\nAt the end of section four, the quote “no publication policy will be perfect. Science is inherently challenging and we must always be willing to accept that a certain proportion of research is potentially false” really stuck out to me because while it seems obvious that some research is wrong, I never really consider that when I read research papers. If a paper is published then that means it’s most likely been peer-reviewed and checked by experts in the field, so the possibility of research being false always seems so low to me. This quote has made me realize that I view research in a very idealistic way without even realizing it. I tend to trust papers that have been published due to their potential peer-reviewed status and while the idea that it’s incorrect may be a thought in the back of my mind, I often don’t entertain it, choosing to trust the researcher. Unless the author of the paper themselves acknowledges their research’s shortcomings or I come across a different paper that criticizes it, I don’t look at papers as critically as I should be, especially if it’s from a field that I have little knowledge about. This quote has revealed a blind spot that I have in regards to research and publications and has made me realize that I need to be more outwardly critical of what I read instead of assuming that the author is right."
  },
  {
    "objectID": "posts/03-confidence_intervals/index.html",
    "href": "posts/03-confidence_intervals/index.html",
    "title": "Project 3: Confidence Intervals",
    "section": "",
    "text": "library(resample)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/03-confidence_intervals/index.html#findings",
    "href": "posts/03-confidence_intervals/index.html#findings",
    "title": "Project 3: Confidence Intervals",
    "section": "Findings",
    "text": "Findings\nThe large sample assumption for 3 of the 6 settings holds, while 3 of the 6 settings violate the large sample assumption. Both settings with p = .03 and p = .45 are violated when n = 8. When tested using the equations n*p and n*(1-p), the results for p = .03 (.24, 7.76) and p = .45 (3.6, 4.4) are less than 10. This also occurs for the n*p equation when p = .03 and n = 57 (resulting in 1.71), making that the third large sample assumption violation. When n = 793, both equations result in values larger than ten for both population proportions (23.79, 769.21 for when p = .03 and 356.85, 436.15 for when p = .45), meaning that the large sample assumption holds. The same goes for the setting n = 57 and p = .45.\nFor the settings that passed the large sample assumption, the coverage rate was close to or at .9 because the confidence interval is 90%. Due to (n = 57, p = .45) and (n = 793, p = .45) both passing the large sample assumption, it can be observed that the coverage rate doesn’t change much as n increases and is equivalent to .9 when rounded. When n = 8, the coverage rates for p = .03 and p = .45 were .222 and .845 respectfully, while the coverage rate for (n = 57 and p = .03) was .807. These three settings failed the large sample assumption and were all around .85 or less, meaning that when the large sample assumption fails to hold, the coverage rate is not .9 or very close to .9. This means that the intervals given by these settings wouldn’t be a good 90% confidence interval, because the amount of coverage isn’t close enough to .90.\nThe average width for p = .45 decreased as n increased, going from .536 when n = 8 to .0581 when n = 793. This decrease can also be seen for p = .03 as the average width goes from .0882 to .0198. Although the large sample assumption fails when (n = 8, p = .45) it does hold for (n = 57, p = .45) and (n = 793, p = .45), which can explain the average width of (n = 57, p = .45) being larger than (n = 793, p = .45). However, due to (n = 8, p = .03) and (n = 57, p = .03) both also failing the assumption, it is hard to tell if that is truly the case. Since t*(s/sqrt(n)) from the confidence interval formula controls the width of the interval, it makes sense that the width would decrease as n increases due to n being the denominator. It could also potentially explain why (n = 8, p = .45) is still larger than (n = 57, p = .45) and why the first two samples sizes have larger average widths than n = 793 when p = .03 despite the large assumption failing to hold."
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "Project 1: Simulation",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/01-simulation/index.html#code",
    "href": "posts/01-simulation/index.html#code",
    "title": "Project 1: Simulation",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-minimum",
    "href": "posts/01-simulation/index.html#normal-minimum",
    "title": "Project 1: Simulation",
    "section": "Normal Minimum",
    "text": "Normal Minimum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 4   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nnorm_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 4\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 4        # population standard deviation\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\nmins_df &lt;- tibble(mins)\n\nnorm_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstn &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstn\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         -5.92          7.16         2.68           5.29"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-maximum",
    "href": "posts/01-simulation/index.html#normal-maximum",
    "title": "Project 1: Simulation",
    "section": "Normal Maximum",
    "text": "Normal Maximum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 4   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nnorm_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 4\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 4        # population standard deviation\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n\nmaxs_df &lt;- tibble(maxs)\n\nnorm_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximums when n =\", n))\n\nsumstx &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstx\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          26.9          7.10         2.66           14.7"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-minimum",
    "href": "posts/01-simulation/index.html#uniform-minimum",
    "title": "Project 1: Simulation",
    "section": "Uniform Minimum",
    "text": "Uniform Minimum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n, theta1, theta2) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(7, 13, length.out = 1000),\n                  xvals_density = dunif(xvals, 7, 13),\n                  pop = \"uniform(7, 13)\")\n## plot the population model density curve\nunif_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Uniform with Theta1 = 7 and Theta2 = 13\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq)\n\ngenerate_samp_min &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(theta1 = theta1, theta2 = theta2, n = n))\n\nmins_df &lt;- tibble(mins)\n\n\nunif_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstum &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstum\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          7.00         0.723        0.851           8.00"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-maximum",
    "href": "posts/01-simulation/index.html#uniform-maximum",
    "title": "Project 1: Simulation",
    "section": "Uniform Maximum",
    "text": "Uniform Maximum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n, theta1, theta2) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(7, 13, length.out = 1000),\n                  xvals_density = dunif(xvals, 7, 13),\n                  pop = \"uniform(7, 13)\")\n\n## plot the population model density curve\nunif_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Uniform with Theta1 = 7 and Theta2 = 13\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\ngenerate_samp_max &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(theta1 = theta1, theta2 = theta2, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\n\nunif_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstux &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstux\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          13.0         0.724        0.851           12.0"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-minimum",
    "href": "posts/01-simulation/index.html#exponential-minimum",
    "title": "Project 1: Simulation",
    "section": "Exponential Minimum",
    "text": "Exponential Minimum\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n## plot the population model density curve\nexp_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with lambda = 0.5\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_min &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(lambda = lambda, n = n))\n\nmins_df &lt;- tibble(mins)\n\nexp_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstem &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstem\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1      0.000154         0.167        0.408          0.408"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-maximum",
    "href": "posts/01-simulation/index.html#exponential-maximum",
    "title": "Project 1: Simulation",
    "section": "Exponential Maximum",
    "text": "Exponential Maximum\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n\n## plot the population model density curve\nexp_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with lambda = 0.5\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_max &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(lambda = lambda, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\nexp_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstex &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstex\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          23.2          5.60         2.37           4.57"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-minimum",
    "href": "posts/01-simulation/index.html#beta-minimum",
    "title": "Project 1: Simulation",
    "section": "Beta Minimum",
    "text": "Beta Minimum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n, alpha, beta) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, 1, length.out = 1000),\n                  xvals_density = dbeta(xvals, 8, 2),\n                  pop = \"beta(8, 2)\")\n\n## plot the population model density curve\nbeta_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Beta with alpha = 8 and beta = 2\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n\ngenerate_samp_min &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(alpha = alpha, beta = beta, n = n))\n\nmins_df &lt;- tibble(mins)\n\nbeta_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstbm &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstbm\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         0.256        0.0110        0.105          0.647"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-maximum",
    "href": "posts/01-simulation/index.html#beta-maximum",
    "title": "Project 1: Simulation",
    "section": "Beta Maximum",
    "text": "Beta Maximum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n, alpha, beta) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, 1, length.out = 1000),\n                  xvals_density = dbeta(xvals, 8, 2),\n                  pop = \"beta(8, 2)\")\n\n## plot the population model density curve\nbeta_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Beta with alpha = 8 and beta = 2\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\ngenerate_samp_max &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(alpha = alpha, beta = beta, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\nbeta_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstbx &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstbx\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         0.999       0.00212       0.0460          0.921"
  },
  {
    "objectID": "posts/01-simulation/index.html#methods",
    "href": "posts/01-simulation/index.html#methods",
    "title": "Project 1: Simulation",
    "section": "Methods",
    "text": "Methods\nFor this project, I created two population models for each of the distributions, one with a red line for the sample minimum, and another with a red line for the sample maximum. I then created histograms showing the sample minimums and maximums for each distribution, and included code that calculated the mean, variance, and standard deviation for the table.\nIn order to answer question 2, I found the CDF and pdf of Exp(.2), then put them into the equations needed to find the pdfs for Ymin and Ymax. After that, I used integration to find the expected values and the variances needed to compute the standard error, after which I then compared to the simulated results. I also made rough sketches of the theoretical graphs, which were then compared to the simulated ones."
  },
  {
    "objectID": "posts/01-simulation/index.html#graphs",
    "href": "posts/01-simulation/index.html#graphs",
    "title": "Project 1: Simulation",
    "section": "GRAPHS",
    "text": "GRAPHS"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#normal-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Normal: Population, Sampling Minimum, Sampling Maximum",
    "text": "Normal: Population, Sampling Minimum, Sampling Maximum\n\nnorm_popmin\n\n\n\n\n\n\n\nnorm_popmax\n\n\n\n\n\n\n\nnorm_min\n\n\n\n\n\n\n\nnorm_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#uniform-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Uniform: Population, Sampling Minimum, Sampling Maximum",
    "text": "Uniform: Population, Sampling Minimum, Sampling Maximum\n\nunif_popmin\n\n\n\n\n\n\n\nunif_popmax\n\n\n\n\n\n\n\nunif_min\n\n\n\n\n\n\n\nunif_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#exponential-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Exponential: Population, Sampling Minimum, Sampling Maximum",
    "text": "Exponential: Population, Sampling Minimum, Sampling Maximum\n\nexp_popmin\n\n\n\n\n\n\n\nexp_popmax\n\n\n\n\n\n\n\nexp_min\n\n\n\n\n\n\n\nexp_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#beta-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Beta: Population, Sampling Minimum, Sampling Maximum",
    "text": "Beta: Population, Sampling Minimum, Sampling Maximum\n\nbeta_popmin\n\n\n\n\n\n\n\nbeta_popmax\n\n\n\n\n\n\n\nbeta_min\n\n\n\n\n\n\n\nbeta_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#table",
    "href": "posts/01-simulation/index.html#table",
    "title": "Project 1: Simulation",
    "section": "TABLE",
    "text": "TABLE\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n5.3\n8.004\n0.4\n0.646\n\n\n\\(\\text{E}(Y_{max})\\)\n14.67\n12\n4.56\n0.922\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n2.67\n0.846\n0.4\n0.106\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.68\n0.829\n2.39\n0.045"
  },
  {
    "objectID": "posts/01-simulation/index.html#questions",
    "href": "posts/01-simulation/index.html#questions",
    "title": "Project 1: Simulation",
    "section": "QUESTIONS",
    "text": "QUESTIONS"
  },
  {
    "objectID": "posts/01-simulation/index.html#question-1",
    "href": "posts/01-simulation/index.html#question-1",
    "title": "Project 1: Simulation",
    "section": "Question 1",
    "text": "Question 1\nBriefly summarise how and compare for each of the above population models. Can you propose a general rule or result for how and compare for a given population?\nFor normal and uniform population models, the standard errors of the sample minimums and maximums are pretty close, with the standard errors for the normal distribution having a difference of only .01 and the standard errors for the uniform distribution having a difference of .017. The beta model’s standard errors are also close with a difference of .061. The exponential model, however, has standard errors that have a large difference. The sample minimum is a small .4 while the sample maximum’s standard error is 2.39, a difference of almost 2. For normal and uniform populations, you can expect the standard errors to be very close together. For beta populations, you can expect the standard errors to be close together but not as close as the normal or uniform populations. For exponential populations, you can expect the standard errors to be far apart. Due to the difference in the exponential population’s standard errors, there can’t be a general rule for all populations. There can be a general rule for three of them: normal, uniform, and beta. That rule would be that the standard errors for the sample minimums and maximums will be close together with a difference of less than one."
  },
  {
    "objectID": "posts/01-simulation/index.html#question-2",
    "href": "posts/01-simulation/index.html#question-2",
    "title": "Project 1: Simulation",
    "section": "Question 2",
    "text": "Question 2\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 5, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * exp(-(2.5) * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\npdf of Ymin: n(1 - F(y))^(n-1) f(y) = 5(1 - e^(-.5y)) ^4 .5e^(-.5y) = 2.5e^(-2.5y)\nE(Ymin) = .4 -&gt; integral of 2.5ye^(-2.5y) (lower bound = 0, upper bound = infinity)\nVar(Ymin) = .32 -&gt; integral of 2.5y^2 * e^(-2.5y) (same bounds as the expected value)\nSE(Ymin) = .32 - .4^2 = .16 = sqrt(.16) = .4\nI find that the theoretical and analytical expected values (.4) and standard errors (.4) are the same. My graph is a bit off, though that is likely because I drew more of a rough sketch, though the simulated graph looks close. It’s likely that with the simulation being generated again, the simulated SE’s and expected values may differ a little bit, but they will still be very close to the theoretical values I found.\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 5, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * exp((2.5) * x) * (1 - exp(-.5))\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\npdf of Ymax: n(F(y))^(n-1) * f(y) =\n5(1-e(-.5x))4 * .5e^(-.5x) =\n2.5e^(-2.5x) * (1 - e(-.5x))4\nE(Ymax) = .467 -&gt; integral of 2.5xe^(-2.5x) * (1-e(-.5x))4 (lower bound of 0, upper bound of infinity)\nVar(Ymax) = 26.71 -&gt; integral of 2.5(x^2) e^(-2.5x) * (1-e(-.5x))4 (lower bound of 0, upper bound of infinity)\nSE(Ymax) = 26.71 - (.467)^2 = 26.5028 = sqrt(26.5028) = 5.148\nThe theoretical expected value is close to the analytical expected value (about .01 off). My theoretical standard error (5.148) was much higher than the analytical standard error (2.39). I think that this shows a high variety in standard errors for Ymax. My graph was different than the simulation, most likely due to my inability to code a correct simulated version of it. However, the x axis seems pretty similar to each other."
  },
  {
    "objectID": "posts/01-simulation/index.html#findings-summary",
    "href": "posts/01-simulation/index.html#findings-summary",
    "title": "Project 1: Simulation",
    "section": "Findings Summary",
    "text": "Findings Summary\nThe sample maximums and minimums of the normal, beta, and uniform distributions have standard errors that are very close, while the standard errors for the exponential distribution’s sample maximums and minimums are very different from each other. As a result, there can be no general rule for all of the distribution’s standard errors, though there can be one for normal, beta, and uniform distributions.\nSimulating the pdfs, expected values, and standard errors of Ymax and Ymin and then comparing them to the theoretical results found that Ymin had very close, if not near identical expected values and standard errors, while Ymax had similar expected values but different standard errors. The difference in standard errors could likely be due to varying varieties of sample maximums. The theoretical graphs were a bit different than the analytical graphs, likely due to coding error, however, the x-axes were pretty similar."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Reflection",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog_326",
    "section": "",
    "text": "Project 1: Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2: Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 3: Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 4: Bayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 5: Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02-estimation/index.html",
    "href": "posts/02-estimation/index.html",
    "title": "Project 2: Estimation",
    "section": "",
    "text": "The normal distribution family is nearly complete. They have their parameter, standard deviation, which they received from the upper beings upon the creation of the normal model, and is all that they could ever ask for. However, they always felt like they were missing something. They couldn’t understand what it was that they were missing, as every time they tried to get to the bottom of it, their minds went blank. Bothered by this confusion, they consulted a magic deity, who revealed the problem to them. It turns out that the normal distribution family was missing an important parameter, the mean! The family was unable to determine what it was that they were missing because the mean was unknown to them. The value assigned to a being registers them to a family, but since they didn’t receive a mean with a value, its existence was forgotten, while the standard deviation was remembered because they knew its value. Distraught at this revelation, the family asked the deity what they could do to find their unknown mean. The deity explained to them that since the mean was not assigned a value immediately upon introduction to the family, they’ll have to find the best estimate for the unknown parameter. It likely won’t be the exact correct value, but if done correctly, the estimate should be good enough. The family was unsure how to do this, so the deity summoned a magic hand to help them.\nThe hand was at least 10 feet tall and had a purple sack around the stub that protruded from the end of its palm. The family and the giant limb exchanged pleasantries before the giant hand dropped the purple sack to the ground. The sack opened upon hitting the ground and an innumerable amount of gems with letters carved into them were revealed. The sack was never-ending, and the family could only make out some of the letters. They were able to see X1, X5, and Xn from where they were standing. The hand explained that the sack of gems represented identically and independently distributed random variables which represent a random sample from the population of the normal model that led to the existence of the family and its standard deviation. The hand explained that the random variable was going to be used to create an estimator needed to help find the estimate. The giant hand then emitted a vibrating sound and the ground shook, causing the sack to explode, leaving a wand in its place. The giant hand lifted the magic wand, introducing it as the Estimator31400. On the wand was a tiny number pad, which the giant hand explained was for changing the values of the function that powers the wand. One of those values pertained to the sample size. The family looked at the wand in awe, asking the giant hand what had just happened. The giant hand explained that it sacrificed the sack of gems to the deities of likelihood. The deities then gifted them the wand in return, having carved it out of one of their great logs. The giant hand went on to explain that when used, the wand derives the maximum likelihood estimator of the unknown mean by taking the random variables (the sack) and applying the magical power of the deity’s great log. The wand took the gems, which had numbers on the back of them that represented observed values from the random sample and maximized the function from the deities of likelihood so that the observed values were the most probable given the estimated parameter. It further explained that this was only one way that they could find an estimate for their missing parameter.\nThe family, still awestruck by this magic wand, asked the giant hand how to use it. The hand showed them that they could use the number pad to change the values of the given function to produce an estimate. As the family plugged the proper numbers in, the wand produced a ball of light. Within the light appeared a number: the estimate. The family felt a rush of excitement, finally able to see an estimate for the mean. The hand explained that the family ought to ensure that the estimator is a good estimator.\nThe air around the family picked up, and a huge gust of wind swept past them, momentarily disorienting them. When they regained their senses, another wand was in front of them, producing its own ball of light and number. The giant hand described this one as the method of moments estimator, which was produced by the deities of momentous. The giant hand explained that it was preferable to have two different estimators to compare, that way the family had more of a choice in how they wanted to find the estimate. In order to compare the two, the estimators had to go through a series of trials. The estimates made within the balls of light disappeared as the balls became blank slates, representing their respective estimators.\nFirst was a test of bias. The goal of this trial is for the estimator to be considered unbiased rather than biased. If an estimator is biased, then the estimate produced would be poisoned and no good for the family. The family looked nervous about the prospect of a bad and poisoned estimate but the giant hand was quick to reassure them that they can make adjustments and find an unbiased estimator and that the estimator can be biased under some circumstances but an unbiased estimator is preferred for the sake of this trial.. First, what occurred was that the “great expectations” of both estimators were found.. The results were put up against the great mewling mu which represented the population parameter, and anything but a result of zero was to be marked for bias. Luckily for the family, both estimators were accepted as unbiased, as it appeared that the great expectations matched the mewling mu.\nThe second trial was a test of variance. Here, the family needed to find the balls of light as they shrunk to match the size of their respective variances and then line them up in a tray. The balls of light exploded into many balls of the same size.The family scrambled after the balls as they rolled around on the floor, only catching them after a few moments. It appeared to the family that the smaller balls of light came from the maximum likelihood estimator. When the family lined up the balls, they found that the smaller ones were closer together, as their size required less distance apart, while the larger balls of light were farther apart and thus a smaller amount of balls were able to fit inside the tray compared to the smaller balls, making the smaller balls the winner of this trial. The variance came with numerical values of course, and when the two were divided in order to find their relative efficiency, the giant hand found that the result came out larger than one, resulting in the maximum likelihood estimator being deemed more efficient as well.\nThe third trial was where variance and bias became entangled with one another, as the balls representing the variance and the biased values of their respective estimators combined with each other to create two giant bright lights. The two lights dimmed as the new values in newly created balls of light emerged, one smaller than the other. The smaller one, belonging to the maximum likelihood estimator, fit snugly in the palm of a family member’s hand, while the larger one was too big for comfort, resulting in the smaller one becoming the favored one.\nThe final trial required the use of the wands, as the family was instructed to change the sample size and observe the change in size of the balls of light. The giant hand wanted to test the consistency of each of the estimators to see how close to the mewling mu they could get. The family observed that as the sample size increased, both of the estimator’s variances became smaller. The giant hand told the family that the decrease in size was a good sign, as the estimators seem to be consistent and therefore close to the mewling mu.\nAt the end of the trials, the family decided that the maximum likelihood estimator is the best estimator of the two, and with a few adjustments to the wand, their hard work is rewarded with their new estimate for their no longer unknown parameter."
  },
  {
    "objectID": "posts/04-bayesian/index.html",
    "href": "posts/04-bayesian/index.html",
    "title": "Project 4: Bayesian Statistics",
    "section": "",
    "text": "library(tidyverse)\n\n\n# second\ntarget_mean &lt;- .7\n\nalphas &lt;- seq(0.1, 66, length.out = 500)\nbetas &lt;- .3 * alphas / .7\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\n\ntarget_var &lt;- .05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.3  19.4 0.00320     0.00000214\n\n\n\ntarget_mean &lt;- .75\n\nalphas &lt;- seq(0.1, 200, length.out = 2000)\nbetas &lt;- .25 * alphas / .75\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\ntarget_prob &lt;- .05\nprob_less_.7 &lt;- pbeta(.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_.7) |&gt;\n  mutate(close_to_target = abs(prob_less_.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1   160.  53.3       0.0500      0.00000851\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha &lt;- 45.3 \ninformative_beta &lt;- 19.4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_priorfrac &lt;- dbeta(ps, informative_alpha,\n                           informative_beta)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_2 &lt;- 160\ninformative_beta_2 &lt;- 53.3\ninformative_prior70 &lt;- dbeta(ps, informative_alpha_2,\n                          informative_beta_2)\n\nnoninformative_alpha_1 &lt;- 1\nnoninformative_beta_1 &lt;- 1\nnoninformative_1 &lt;- dbeta(ps, noninformative_alpha_1,\n                             noninformative_beta_1)\n\nplot_df &lt;- tibble(ps, informative_priorfrac, noninformative_prior,\n                     informative_prior70) |&gt;\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Prior Distributions\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha_post1 &lt;- 45.3 + 56 \ninformative_beta_post1 &lt;-  84 - 56 + 19.4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_postfrac &lt;- dbeta(ps, informative_alpha_post1,\n                           informative_beta_post1)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_post70 &lt;- 160 + 56\ninformative_beta_post70 &lt;- 84 - 56 + 53.3\ninformative_post70 &lt;- dbeta(ps, informative_alpha_post70,\n                          informative_beta_post70)\n\nnoninformative_alpha_postf &lt;- 56 + 1\nnoninformative_beta_postf &lt;-  84 - 56 + 1\nnoninformative_postflat &lt;- dbeta(ps, noninformative_alpha_postf,\n                             noninformative_beta_postf)\n\nplot_df &lt;- tibble(ps, informative_postfrac,\n                     informative_post70, noninformative_postflat) |&gt;\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Posterior Distributions\")\n\n\n\n\n\n\n\n\n\n# flat posterior\nalpha_flat &lt;- 56 + 1\nbeta_flat &lt;- 84 - 56 + 1\npost_mean_flat &lt;- alpha_flat / (alpha_flat + beta_flat)\nlower_bound_credible_flat &lt;- qbeta(.05, 57, 29)\nupper_bound_crebile_flat &lt;- qbeta(.95, 57, 29)\n\nflat &lt;- tibble(post_mean_flat, lower_bound_credible_flat, upper_bound_crebile_flat)\nflat\n\n# A tibble: 1 × 3\n  post_mean_flat lower_bound_credible_flat upper_bound_crebile_flat\n           &lt;dbl&gt;                     &lt;dbl&gt;                    &lt;dbl&gt;\n1          0.663                     0.577                    0.744\n\n# 46 out of 66 informative\nalpha_46 &lt;- 45.3 + 56\nbeta_46 &lt;- 84 - 56 + 19.4\npost_mean_46 &lt;- alpha_46 / (alpha_46 + beta_46)\nlower_bound_credible &lt;- qbeta(.05, 101.3, 88)\nupper_bound_credible &lt;- qbeta(.95, 101.3, 88)\n\npost_46 &lt;- tibble(post_mean_46, lower_bound_credible, upper_bound_credible)\n\npost_46\n\n# A tibble: 1 × 3\n  post_mean_46 lower_bound_credible upper_bound_credible\n         &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1        0.681                0.475                0.594\n\n# 75% prior\nalpha_75 &lt;- 160 + 56\nbeta_75 &lt;- 84 - 56 + 53.3\npost_mean_75 &lt;- alpha_75 / (alpha_75 + beta_75)\nlower_credible &lt;- qbeta(.05, 216, 81.3)\nupper_credible &lt;- qbeta(.95, 216, 81.3)\n\npost_75 &lt;- tibble(post_mean_75, lower_credible, upper_credible)\n\npost_75\n\n# A tibble: 1 × 3\n  post_mean_75 lower_credible upper_credible\n         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1        0.727          0.683          0.768\n\n\nThe purpose of this project is to attempt to find the probability of Rafael Nadal winning a point on his own serve against Novak Djokovic. To do so, we are creating 3 prior distributions, two of which are based on information from a previous game and a claim made by a sports announcer, and one that is based on no information at all. The first two distributions are called informative distributions, and the other one is a non-informative prior distribution. I will be using a flat prior (Beta(1,1)) for my non-informative prior. After finding the parameters for each prior, I will take the observed data and combine it with the prior distributions to create the posterior distributions.\nFor the first informative prior, I divided the points won out of total points served (46/66) to get a mean needed in order to use R code to compute the parameters for the informative priors. The variance was already given, so all that was needed was a target mean, which ended up being the result of 46/66. The resulting parameters were determined to be adequate after “testing” them using the beta mean equation to see if the mean matched the target mean, which it did. The assumption that I made when making this prior was that the game that this information was taken from is independent of all other games. I also assumed that the two players had a relatively equal skill level.\nFor the second informative prior, I used the 75% as the mean, since the curve would need to be centered at .75 because it’s the proportion of points won (according to the sports announcer). To find the parameters, I used code that used a target probability rather than variance since the information given was that Nadal scored no less than 70% of the time which is just P &gt;= .7. I decided to increase the value in the sequence function from 100 to 200 because that gave parameters that had a probability equal to .05, which is the target probability that I chose. If I kept the sequence at 100, the code would only produce parameters that had around a .09 probability of no less than 70%. I felt that .09 wasn’t a small enough probability because it was too close to .1, so I chose parameters that had a lower probability. I also prefer the parameters from the .05 target probability because with the sequence at 200, the probability is almost exactly .05, compared to when the probability was around .094. For this prior, I also assumed that both players had similar skill levels and were therefore evenly matched when the games were played.\nFor the non-informative prior, I chose to do a flat prior using Beta(1,1), since the beta distribution allows you to model a quantity between 0 and 1.\nAll of the posterior distributions are different from each other because they all had different parameters and target means due to the difference in information (or lack thereof) provided for each prior distribution. Each posterior distribution has a different mean, so they’re centered at different places on the graph. The flat prior had the lowest mean, which makes sense given that the prior distribution was only Beta(1, 1). It also makes sense that the sports announcer’s posterior has the highest mean because it began with the highest mean and each distribution shifted the same amount, so the mean would remain the highest.\nAll three also saw a decrease in variance compared to their prior distributions, due to an increase in both beta and alpha. The informative posterior distribution that used the sports announcer’s information has the lowest variance out of the three curves. This could likely be due to the sports announcer’s information being more about Nadal’s overall points won percentage against Djokovic compared to the first informative prior that uses data from only one match. Therefore, we are receiving more information from the sports announcer, so the variance is smaller since there’s less uncertainty. The posterior distribution from the flat prior has the most variance because there’s no information being used. Less variance means that there’s a lower variability, meaning that the values are closer together, which could also stem from knowing more information because it helps with accuracy, therefore it would make sense that the values are closer together.\nThis certainty is also shown in the credible intervals, where the sports announcer’s posterior distribution had the smallest difference between the upper and lower bounds of their 90% credible intervals (a .09 difference) compared to the other informative posterior distribution that had a difference of .119, and the flat prior that had a difference of .167.\nIf I had to choose one posterior distribution, I would choose the one that used the previous match for the informative prior because the mean is close to that of the observed data. The variance might be larger than the sports announcer’s posterior, but it seems like the observed data would actually fit under the curve, which I think is more important than a lower variance. The credible interval is not wide enough for it to be an issue either.\nThe resulting posterior distributions showed that increase both alpha and beta decreased the variance, and a change in mean shifted the distribution curves to the left. The informative posterior distribution that used the most information had the lowest variance and width of its credible interval, but also the highest mean which is likely from the already high target mean used to find the parameters of the prior distribution. The flat prior had the widest variance and smallest mean due to it being non-informative and only having prior distribution of (1,1). The second informative prior that used the previous match saw a decrease in both variance and mean. I think that it’s the best one to use because it seems like it includes the observed data under its curve (unlike the other informative posterior) and had a lower variance than the flat distribution."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]