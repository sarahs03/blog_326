---
title: "Project 5: Hypothesis Testing"
---

# 1.

I think the author means that people will focus less on p \< .05 and the significance of their results and instead focus on actually analyzing and discussing their results. De-emphasizing the p-value and statistical significance would allow for more discussion of less than perfect results. Instead of fixating on results that fit within the constraints of p \< .05, researchers could discuss results that may have higher p-values. Doing so would allow for more analysis into what worked and what didn’t and how such results may have occurred, thus improving overall knowledge and potentially even improving the test or experiment used. I also think that getting rid of the use of statistical significance would result in researchers having to consider other means of statistical analysis and inference, forcing them to question the effectiveness and usefulness of different statistical tools, compare models, and analyze their assumptions about the data and results, which is statistical thinking. Instead of basing their explanations primarily around significance, they may analyze their data to see what is actually occurring rather than what is “significant”. I think that to embody statistical thinking, it is required to consider any potential biases and view the problem from a neutral standpoint. For example, it may be a popular sentiment that package theft has been on the rise due to so many neighbors having their packages stolen from their porch. However, if you were to take data on porch piracy and analyze it via graphing and modeling, you would see that porch piracy has actually been decreasing, and that the belief of an increase may come from exposure bias. Statistical thinking isn’t just about acknowledging bias and conducting analyses to disprove common beliefs, however. It’s also about applying statistical tools and techniques to solve problems. For example, politicians and statisticians may work together to decrease the amount of opiate overdoses by using statistics from previous years about emergency response times, education, and the prevalence of narcan use in responding to opiate overdoses. Statisticians could use different tools such as modeling that may result in suggestions of an increase in narcan supply, the necessity of faster emergency response times, and an increase in education. Politicians could then use that information to implement different policies to do such a thing. However, results that may occur from such modeling are not foolproof, and in order to completely embody statistical thinking, it’s necessary to acknowledge the potential shortcomings and limits of models and analyses.

# 2.

The author means that treating p-values as a means of determining what is significant and what is insignificant is pointless because this idea of significance comes from a misunderstanding of its original use. Significance is often associated with usefulness, importance, and probability while insignificance is associated with unimportance and improbability. The intention of the use of the word significance was to indicate whether or not more scrutiny was needed - it was never meant to be used in the way that it is used today. Now, p-values are used as a part of determining the rejection of the null hypothesis in hypothesis testing where questions of significance are regularly brought up and the rejection or failure or rejection involves explanations in determining whether or not something was statistically significant. When the author mentions the dichotomization of p-values, they are talking about the categorization of “significant” and “not significant.” The issue with the dichotomization of the p-value is that it incorrectly establishes an authority of the p-value where it can be used to determine the significance or lack thereof in a way where significance is misunderstood as providing proof of plausibility and importance of a result. It contributes to black and white views of results, where one side is deemed worthy of consideration and the other is deemed unworthy and disregarded. This can result in researchers prioritizing certain results when the disregarded results may have provided interesting insight into their research. It creates this idea that significance should be valued above all when there are many other useful statistical inferences that have been disregarded alongside unworthy results.

# 3.

I agree that p-values shouldn’t have to pass a threshold in order to choose which results to present or highlight because as the paper points out, it could result in bias since the results that fit the proper threshold will get published and reported, leaving out other results that may be of interest. That interest may not be due to “correctness” (i.e. fitting a desired outcome) or what works best, but rather something that can be learned. I think that it’s useful to highlight results that have occurred due to an error or a deviation from the intended experiment or process, as it can help researchers understand what’s happening, how, and what aspect impacts or causes certain results. With that being said, I don’t think that there’s a one size fits all solution to determining what gets presented in publishing, rather it’s a case by case basis. I believe that results that reveal information about the data, regardless of whether they were the desired outcome or the product of a mistake, would serve as noteworthy inclusions given that they help researchers further understand a topic or process. When it comes to “bad” results, it’s always good to look back on what went wrong and how to adjust the study or experiment in the future. I think that reproducibility is an important aspect as well, as it allows others to see where results have come from and further their understanding as to why and what’s been impacted. I think that there are some cases, such as the FDA’s, where p-values can be used to determine what gets published, if only until a better solution for interpreting the results is found since some organizations and researchers have become very reliant on the p-values.

# 4.

I agree that there’s no one size fits all approach to statistical inference in scientific research because there are too many fields of science and too many types of research to come up with one approach that works well for all of them. Each field has its own standards, with some standards being more similar than others. There is bound to be some conflict in what is appropriate and what is inappropriate in certain fields. I believe that what works best is dependent on the specific study or experiment being done, as some statistical tools and methods might be of better use than others. The FDA example given further in the reading is a good example of this. While it may be possible for them to move away from their reliance on p-values, it’s not an easy task considering that they have been doing it for such a long time and that it’s worked for them. It’s much easier to establish general principles like ATOM and allow the researcher to use what works best for their research, as long as they provide extensive justifications for what they are choosing to do and why.

# 5.

I think that statistical thoughtfulness means putting everything into consideration. It’s considering all of the prior information and context of the data you are looking at, potential results of the study/experiment, careful designs of your study/experiment, and the utilization of various methods of analysis when looking at results. As said in the text, it’s also about “investing in producing solid data” (page 3, section 3).To demonstrate statistical thoughtfulness, researchers should ask questions pertaining to their data. For example, they should be considering whether or not they made the correct assumptions when analyzing their data, question the precision of their estimates, and whether or not the study was adequately designed. Statistical thoughtfulness to me also means being very thorough in your analysis and study, considering possible outcomes and being able to justify what you’re doing and why. It also involves putting thought into how results and data are conveyed, and their implications. Another way to demonstrate statistical thoughtfulness is by making sure that your data and results are explained in ways that are accessible to your audience. An example of this is seen in 3.2.4, where some authors suggest replacing the words significant and confidence with compatibility so that the terms aren’t misunderstood. Overall, I consider statistical thoughtfulness to be very similar, if not nearly identical to statistical thinking, with statistical thoughtfulness being more thorough and purposeful in what is being done and more research related, while statistical thinking is better suited for non-resarch occurrences and applications.

# 6.

I think that the authors believe that the problem is that people see words like significance and confidence and make the wrong associations. People associate significance with importance and necessity and confidence with certainty, so when they are looking at confidence intervals, they’re seeing an interval of “correct” data with the incorrect and useless values left out of the interval and to be considered no longer. In combination with the prioritization of a small p-value due to a misconstrued understanding of significance, the author believes that there’s a risk of overconfidence which I believe may stem from the acceptance of confidence intervals that are too narrow simply because it contains a certain value. I think the overconfidence is not necessarily about the narrowness of the interval, but also in regards to misunderstood interpretations of confidence intervals, where the intervals are treated less as estimates and more as significant values. The authors introduced the term compatibility to try and rid p-values and confidence intervals of misunderstood associations and to encourage people to view confidence intervals as producing estimates that are compatible with the data under the created model instead of values that the data should fall into. It’s a similar situation with p-values, where compatibility can be used to shape the view of p-values as “measuring compatibility between hypothesis and data” (section 3.2.4) instead of p-values as determining significance. I think that the overall goal is to view confidence intervals and p-values in relation to the data instead of as a descriptor of the data.

I agree that there may be an issue with people misunderstanding the words significance and confidence, resulting in the misunderstanding of confidence intervals and p-values. However, I don’t really agree that the solution is to change the wording because I feel like compatibility may still be misunderstood in a similar way to the words significance and confidence, thus resulting in the similar treatment. I think the better course of action is to provide more in depth explanations about what is actually happening and what the p-values and confidence intervals are actually telling you and what they actually represent. Changing the words to compatibility may help with some context, but I think in order to get people to fully understand what is going on, you need to explain it to them instead of relying on implicit understanding.

# 7.

At the end of section four, the quote “no publication policy will be perfect. Science is inherently challenging and we must always be willing to accept that a certain proportion of research is potentially false” really stuck out to me because while it seems obvious that some research is wrong, I never really consider that when I read research papers. If a paper is published then that means it’s most likely been peer-reviewed and checked by experts in the field, so the possibility of research being false always seems so low to me. This quote has made me realize that I view research in a very idealistic way without even realizing it. I tend to trust papers that have been published due to their potential peer-reviewed status and while the idea that it’s incorrect may be a thought in the back of my mind, I often don’t entertain it, choosing to trust the researcher. Unless the author of the paper themselves acknowledges their research’s shortcomings or I come across a different paper that criticizes it, I don’t look at papers as critically as I should be, especially if it’s from a field that I have little knowledge about. This quote has revealed a blind spot that I have in regards to research and publications and has made me realize that I need to be more outwardly critical of what I read instead of assuming that the author is right.
