---
title: "Reflection"
---

The first mini-project had us conduct a simulation to investigate an observation made during a recap task where SE(Ymin) is around the same value as SE(Ymax) when n = 5. Through this project, I discovered that this was the case for the normal and uniform distributions given, but not for the exponential and beta distributions. This project relied a lot on graphs, as the symmetry of the points representing the samples on the population graphs helped with coming up with the rule that the project asked me to propose. The graphs helped visualize what was occurring at n = 5 for the given distributions, and provided a way to determine a potential pattern for the distributions that had SE(Ymin) and SE(Ymax) match and a pattern for the distributions whose standard errors weren’t close. In this case, the distributions with close standard errors had symmetrical points on the population graph, while the distributions with different standard errors had asymmetric points under the population curve. The use of graphs in this project ties in with the fourth mini-project, where I compared the prior and posterior distribution curves, finding changes in the mean and variances after updating the prior distributions. These two projects had me practice reading graphs and drawing conclusions based on changes in previous graphs like in the fourth project, or the location of the samples on the population graph like in the first project. Similarly for the third mini-project, instead of using graphs, I had to use the table of collected data and draw conclusions from patterns that I noticed in the average width and coverage rates of different sample sizes and population proportions. For the first mini-project I had also created a table of standard errors and expected values for the corresponding distributions given. The goal of both of these projects was to analyze the differences and similarities between the respective values (so, differences in coverage rates, average widths, and standard errors) and hypothesize why certain differences were closer together or farther apart. With the first project, a rule could be proposed based on the pattern of the points on the population graphs, while the conclusion for the third project was that the small sample size caused a coverage rate that wasn’t equal to the confidence percentage and resulted in a different average width due to the sample size being included in the width equation. This project was one of the first things we did involving R and statistical simulations and the third and fifth mini-projects are a continuation of using statistical simulation. In each project, we used simulation to investigate different things, such as violations in assumptions and the probability of a tennis player scoring a point. Following this project, we spent the rest of the course using statistical simulations in our labs to practice new concepts, analyze data, and answer questions. In the hypothesis testing unit, we did a lab where a researcher wanted to assess whether or not there was evidence that a drug helped lower blood pressure. In that lab, we simulated 10,000 hypothesis tests and recorded each p-value to calculate the empirical power. The null was rejected after finding at least one p-value that was less than alpha. An example that used sampling distributions is lab 3.2, where we simulated 1,000 confidence intervals from a normal distribution with the purpose of finding the coverage rate, which is the fraction of the intervals that contain the parameter from the distribution. We also used statistical simulation to simulate thousands of means from one sample to find the confidence interval of the population mean using bootstrapping.

My biggest take-away from this project was that statistical simulation isn’t only for things that require thousands of samples, means, etc. We can also use simulation to investigate something as “simple” as differences and similarities between the standard errors of the sample maximum and minimums of certain distributions. In class, we did a lot of simulations like the bootstrapping lab or the empirical power lab where it was necessary to generate thousands of samples in order to find the empirical power or find the confidence interval of a population based on one sample. This project was unique in that the parameters were known, which made the simulation more simple since I didn’t have to do anything to find the parameters. Though I didn’t have to deal with unknown parameters, it was still difficult to make the graphs using the distribution, as the code was a bit confusing to me. The use of graphs was a major aspect of this project, as they gave useful insight as to why some standard errors had a larger difference than others when comparing the sample maximum and minimums. This project in particular emphasized the importance of analyzing graphs, rather than just relying on the table of results. I further saw this emphasis throughout the remainder of the course, where graphs were not only used to visualize the data, but to also determine what may occur when we complete our lab. The example that most stuck out to me was when we created a power curve in the last lab of the semester. We used the curve to analyze what happened if n increased or decreased. Using the graph, we were able to tell that an increase in n made it so that it was much easier to detect a difference. The use of graphs may be a major aspect of statistical simulation, though they are not the only part. Statistical simulation can also produce different statistics that we are looking for, such as the standard errors of the sample maximums and minimums for this project.

For the second mini project, we were required to write a story that conveys the meaning of terms like estimate, likelihood, bias, and variance and demonstrate how they connect with each other. This project required the use of terms that were most used during the estimation unit, though these terms were not specific to that one unit. Terms such as variance, estimate, estimator, parameter, random variable, random sample, and likelihood were used frequently throughout the course, which makes this project the most connected to the rest of the course. This project ties to the other content of the course by ensuring that I understand what the terms I was using actually mean and how they connect to each other. For example, I connected random variable with estimator by having a character in my story explain that random variables are used to create an estimator which in turn is used to create an estimate of an unknown parameter. Following the first unit, we encountered a lot of distributions that had unknown parameters, as it is common that we don’t know at least one parameter in a distribution. Since this story revolved around explaining the process of finding an estimate for an unknown parameter, this ties in with the Bayes unit, where we need to find a prior distribution in order to obtain a posterior distribution, which can then be used to find the Bayes estimate. Throughout the course, we used the majority of the vocabulary that we were required to include in the story. Likelihood was used in the hypothesis testing section to conduct likelihood ratio tests to find which model of the given null and alternative hypotheses fit the best, and when to reject the null hypothesis. Likelihood in the Bayes section was used to help derive the posterior distribution. For this project, I was required to use a specific probability distribution, which we worked with throughout the course. The sampling distribution section served as a review and introduction to this, though the parameters were known to us. However, even though they were known, we still used variance to analyze graphs, like we did in other sections, such as the empirical power lab of the hypothesis testing section where we discussed the impact on delta if the variance was increased or decreased. Due to having to create a story that demonstrates understanding rather than simply defining these terms and explaining their connection to each other, there was a lot of thought that went into figuring out how to write a story that conveys these things. I had to think about how to write a story that included all of these terms, their definitions, and connections in a covert way while also maintaining the flow of the story. I think that the article that I read for the fifth mini project ties in with this aspect, as the authors spent a lot of time discussing how to write interpretations of data in a way that can’t be easily misunderstood due to choices in wording. Both of these projects dealt with thinking about the best ways to convey information to an audience. With this project, it was how to convey my understanding in a non-obvious way (i.e. simply defining the terms); with the fifth mini-project, it was considering the use of the word compatibility in place of significance and confidence. This careful consideration of how to convey my understanding can also tie to the fourth mini project, where I had to justify the way I prior distributions I created instead of just letting the code I used speak for itself. I also had to consider the different posterior distributions and explain which one I would choose, forcing me to display my understanding of the differences in each posterior distribution. If my explanations are not clear, then there is a risk of my justifications for my prior distributions being misunderstood, and if someone reading it were to try to replicate how I produced them, they could likely come up with the wrong distribution. If my reasoning for choosing the posterior distribution were unclear, it could come off as though I don’t understand what makes a good posterior distribution, and that I was just guessing. How to convey my understanding of the topic at hand is one of my biggest take-aways, as it’s very easy to just define the terms and provide one sentence explanations of how they connect. In order to demonstrate true understanding, you should be able to apply these terms in different contexts and use those contexts to demonstrate their meanings and connections. After completing the course, I realized why this particular unit was chosen for this type of project. We see most of the vocabulary words in every section following the first one because there’s always one parameter that is unknown in the distribution, so it’s important to fully understand what these terms mean and how they connect. If we don’t fully understand, or we only understand the surface level meanings and connections akin to the phrase “I know what it means but I can’t define it for you”, it will be much harder to grasp what we are doing in each exercise that uses these terms. For example, if we don’t understand what likelihood means and how it connects to estimation, it would be harder to understand why we were using likelihood in the Bayes unit or the hypothesis testing unit.

For the third mini-project, I investigated what happened to confidence intervals if the large sample assumption was violated. It was discovered that the coverage rates would not be equivalent or close together when the population proportion is changed. This meant that when a violation occurred, the coverage rate for a 90% interval would not be close to the required .9 at certain population proportions. This project was a demonstration of the necessity of passing assumptions. Throughout the course, we often assumed that the required assumptions were not violated. In the Bayes unit, we did a lab with women’s hockey data, and it wasn’t until the very last question of the lab that we had actually considered the assumptions we made when conducting the analysis. When we did exercises in class, we mainly focused on assumptions of independence and sample size. Even in some of the labs, we didn’t explicitly check assumptions outside of normality and sample size – we just assumed that the assumptions passed. In the hypothesis testing section, we were more explicit, and checked the assumptions before conducting the proper hypothesis test. This explicit checking of assumptions ties in with the fifth mini-project, where I discussed the concept of statistical thoughtfulness. The author of the article used the questioning of assumptions as an example of statistical thoughtfulness, stating that a statistically thoughtful researcher would question whether or not the study passed the proper assumptions. This mini-project demonstrates the importance of ensuring that the assumptions pass, as a consequence of a failed large sample assumption is that the confidence intervals won’t have the correct amount of coverage rate, rendering the confidence interval a bad one. If the confidence interval isn’t good, it won’t provide the proper interval, which would lead to a poor analysis. The article in the fifth mini-project expressed a lot of concern about people misunderstanding results of studies, particularly due to the wording of interpretations, one of those interpretations involving confidence intervals. I think that this is also a connection to this mini-project, because a confidence interval produced from a too small sample size would create a false understanding of the intended subject of analysis. The biggest take-away of this mini-project is the importance of ensuring that assumptions are not violated, as the results it produces will be incorrect and could lead to misinformation. If the assumption is violated, it could result in an error when conducting analyses or using equations to find something like the test-statistic, which uses the sample size. When an error occurs in any aspect of an equation or a process such as the likelihood ratio test, it can become impossible to actually solve or complete the equation or test, or will produce an incorrect result that can affect the rest of the analysis. For example, if an error occurs during the likelihood step of the likelihood ratio test, it becomes impossible to derive the test or come to the correct result. In certain circumstances the violation of the large sample assumption is made known through the coverage rate and average width of confidence intervals or the test-statistic equation. In other circumstances, the violations are not as obvious unless you are able to identify that something is wrong with your result. This circumstance is particularly troublesome, because there’s a likelihood that the error isn’t caught, allowing the statistician to accidentally present incorrect information.

For the fourth mini-project, I had to create non-informative prior distributions and informative prior distributions based on information provided about tennis player Nadal’s winning points. I then used updated information to create posterior distributions for the purpose of finding the probability that Nadal wins a point. The reason for creating these prior and posterior distributions was that there were no known parameters. Though this project uses Bayesian statistics instead of frequentist statistics, unknown parameters were a regular occurrence after the first section. Through this, this project ties in with the work we did with unknown parameters. In this project, I used prior information to create a probability distribution that captures the information. In the estimation section, we used methods of moments and maximum likelihood estimation to estimate unknown parameters. We also saw unknown parameters in the confidence interval section, where we used bootstrapping in order to find the confidence interval of an unknown population parameter and the pivot method to find the confidence interval of an unknown parameter. In the hypothesis section, we derived the most powerful test using a given distribution with a missing parameter, although we were given values for the null and alternative hypotheses. This project provided another method of dealing with unknown parameters using a different statistical philosophy, as all of the other cases we encountered used frequentist methods. Instead of using likelihood functions or pivoting for this project, I used statistical simulation to find the alpha and beta of both prior and posterior distributions. I also used statistical simulation in the first mini-project to find and compare the standard errors of the sample maximums and minimums. Like that project, I also used graphs to analyze the data, finding indications of changes in variance and mean after updating the prior distributions using information from the 2020 French Open data. Another thing that I had to do for this mini-project was justify my prior distributions, which ties in with the fifth mini-project, where I was asked what I thought statistical thoughtfulness meant. With context from the article, I mentioned that it involved justifying everything that is done during a study or experiment. In this case, I justified the decisions I made when I created the informative priors, opting to use the given fraction (46/66) and 75% as the respective inputted means in the simulation I used to produce the alphas and betas. This was an exercise in statistical thoughtfulness, as not only it required me to really think about what I was actually doing and why. My biggest takeaway from this project is that even if you don’t know the parameters, you can still make a distribution. In frequentist statistics we saw the use of the maximum likelihood to find an estimate of an unknown parameter. In Bayesian statistics and this project, you can make prior distributions and update them when more information arises. Like in frequentist statistics, some posterior distributions (or estimates, in frequentist statistics) are going to be better than others. The more information you have, the better the resulting prior and posterior distributions. In this project, however, when I was choosing what posterior distribution to use, I had to decide between the lower variance or a mean that was closer to the observed mean. Through this exercise, I learned that there are going to be some situations where you need to decide between two different desirable statistics, such as mean or variance, when choosing the best distribution. Similarly, this is seen in frequentist statistics with bias-variance trade-off when determining the best estimate.

For the last mini-project, I had to read an article titled Moving to a World Beyond p \< .05. This mini-project had me consider what the author meant when they brought up the terms “statistical thinking” and “statistical thoughtfulness”. Both of these terms apply to the field of statistics as a whole, as a good statistician is statistically thoughtful. Throughout this class, we employed statistical thinking by considering things like the best prior distribution to use in the Bayes unit, as well as questioning what assumptions were made during the process of finding a posterior distribution. During the confidence interval unit, we also employed statistical thoughtfulness when choosing the proper pivot quantity and justifying our choice through the rules required for a correct pivot quantity. Additionally, there was a question about the reading pertaining to the article’s author considering using the word compatibility in place of the word confident when interpreting confidence intervals. This part also ties into the confidence interval unit, as I had to consider the possibility of my interpretations being misunderstood due to people taking the use of the word confident to mean certain and correct. Similarly, this can be applied to the Bayes unit, where we interpreted credible intervals. The difference in interpretations of credible and confidence intervals was addressed in class, and we discussed how the interpretations for credible intervals were easier to understand since we use the term “probability” instead of “confidence”. Overall, this project had me considering the way that I analyzed and interpreted data. It put emphasis on choice of wording, particularly the use of “statistically significant” and “confident”. The authors wanted to minimize misunderstandings and misinterpretations as much as possible. I think that this aspect of the article, along with the questions associated with these considerations tie in with the second mini-project, where I was tasked with writing a meaningful story that not only demonstrated my understanding of specific terms, but also how these terms connected with each other. As a result, I had to put a lot of thought into how to use the required terms in a way readers could understand how they connected. Both projects required thought about conveying information and minimizing misunderstanding. Though the article talked a lot about word choices, it also discussed de-emphasizing the use of p-values to evaluate a result and whether or not to share the result in published research. There were a couple of related questions pertaining to statistical inference and how to determine which results to present in research. The conclusion that I came to was that there was no one-size fits all approach to either question. In the hypothesis testing section for example, we used the likelihood ratio test to compare models using the null and alternative hypotheses given. When it comes to choosing which results to present, I responded to the corresponding question in this mini-project with a suggestion that including mistakes is just as valuable as including desired results as they can reveal new information about what’s being studied. The third mini-project is a demonstration of this, as we investigated what occurs when the large sample assumption is violated. Through this project, we saw the impact that the violation had on the average width and coverage rate of a confidence interval. Due to this violation (and therefore mistake) we were able to see how the sample size impacted the width and coverage rate. Even though the assumptions were violated, we were still able to learn something about confidence intervals, which is just as important as discussing the results that may have occurred if the large sample assumption passed. 

My biggest take-away from this project is that you don’t have to be perfect in the studies you conduct, as there are still plenty of things to be learned from mistakes that are made. We saw this in the third mini-project, where we observed the effects of a violated large sample assumption. In class, we often discussed mistakes that were made and even if it wasn’t a mistake that caused a significant change, it was still a learning lesson, as we learned about some common mistakes that could be made when solving problems. For example, when we were deriving the most powerful test, we learned that making a mistake in the algebra or likelihood step makes it impossible to derive the correct result. Another thing I learned from this project was that statistical thoughtfulness comes in many shapes and forms. When we were testing the bias and consistency of our estimates in the estimation section, we were practicing statistical thoughtfulness. When we were writing the interpretations of our confidence intervals, credible intervals, and hypothesis tests, we were practicing statistical thoughtfulness. Each project that we had this semester was an exercise in different forms of statistical thoughtfulness, be it an exploration of assumption violation, conveying the connections and meanings of specific terms, further investigating an observation made in class, justifying the creation of prior distributions using given information, or considering which results to present.
