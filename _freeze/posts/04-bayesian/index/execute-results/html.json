{
  "hash": "e71144805a4bf5fc6ca5f0c84297a4c4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Project 4: Bayesian Statistics\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# second\ntarget_mean <- .7\n\nalphas <- seq(0.1, 66, length.out = 500)\nbetas <- .3 * alphas / .7\n\nparam_df <- tibble(alphas, betas)\nparam_df <- param_df |> mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\n\ntarget_var <- .05657^2\nparam_df <- param_df |> mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |> filter(dist_to_target == min(dist_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   <dbl> <dbl>   <dbl>          <dbl>\n1   45.3  19.4 0.00320     0.00000214\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_mean <- .75\n\nalphas <- seq(0.1, 200, length.out = 2000)\nbetas <- .25 * alphas / .75\n\nparam_df <- tibble(alphas, betas)\nparam_df <- param_df |> mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\ntarget_prob <- .05\nprob_less_.7 <- pbeta(.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_.7) |>\n  mutate(close_to_target = abs(prob_less_.7 - target_prob)) |>\n  filter(close_to_target == min(close_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas prob_less_.7 close_to_target\n   <dbl> <dbl>        <dbl>           <dbl>\n1   160.  53.3       0.0500      0.00000851\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nps <- seq(0, 1, length.out = 1000)\n\ninformative_alpha <- 45.3 \ninformative_beta <- 19.4\n\nnoninformative_alpha <- 1\nnoninformative_beta <- 1\n\ninformative_priorfrac <- dbeta(ps, informative_alpha,\n                           informative_beta)\nnoninformative_prior <- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_2 <- 160\ninformative_beta_2 <- 53.3\ninformative_prior70 <- dbeta(ps, informative_alpha_2,\n                          informative_beta_2)\n\nnoninformative_alpha_1 <- 1\nnoninformative_beta_1 <- 1\nnoninformative_1 <- dbeta(ps, noninformative_alpha_1,\n                             noninformative_beta_1)\n\nplot_df <- tibble(ps, informative_priorfrac, noninformative_prior,\n                     informative_prior70) |>\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |>\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Prior Distributions\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nps <- seq(0, 1, length.out = 1000)\n\ninformative_alpha_post1 <- 45.3 + 56 \ninformative_beta_post1 <-  84 - 56 + 19.4\n\nnoninformative_alpha <- 1\nnoninformative_beta <- 1\n\ninformative_postfrac <- dbeta(ps, informative_alpha_post1,\n                           informative_beta_post1)\nnoninformative_prior <- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_post70 <- 160 + 56\ninformative_beta_post70 <- 84 - 56 + 53.3\ninformative_post70 <- dbeta(ps, informative_alpha_post70,\n                          informative_beta_post70)\n\nnoninformative_alpha_postf <- 56 + 1\nnoninformative_beta_postf <-  84 - 56 + 1\nnoninformative_postflat <- dbeta(ps, noninformative_alpha_postf,\n                             noninformative_beta_postf)\n\nplot_df <- tibble(ps, informative_postfrac,\n                     informative_post70, noninformative_postflat) |>\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |>\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Posterior Distributions\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# flat posterior\nalpha_flat <- 56 + 1\nbeta_flat <- 84 - 56 + 1\npost_mean_flat <- alpha_flat / (alpha_flat + beta_flat)\nlower_bound_credible_flat <- qbeta(.05, 57, 29)\nupper_bound_crebile_flat <- qbeta(.95, 57, 29)\n\nflat <- tibble(post_mean_flat, lower_bound_credible_flat, upper_bound_crebile_flat)\nflat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  post_mean_flat lower_bound_credible_flat upper_bound_crebile_flat\n           <dbl>                     <dbl>                    <dbl>\n1          0.663                     0.577                    0.744\n```\n\n\n:::\n\n```{.r .cell-code}\n# 46 out of 66 informative\nalpha_46 <- 45.3 + 56\nbeta_46 <- 84 - 56 + 19.4\npost_mean_46 <- alpha_46 / (alpha_46 + beta_46)\nlower_bound_credible <- qbeta(.05, 101.3, 88)\nupper_bound_credible <- qbeta(.95, 101.3, 88)\n\npost_46 <- tibble(post_mean_46, lower_bound_credible, upper_bound_credible)\n\npost_46\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  post_mean_46 lower_bound_credible upper_bound_credible\n         <dbl>                <dbl>                <dbl>\n1        0.681                0.475                0.594\n```\n\n\n:::\n\n```{.r .cell-code}\n# 75% prior\nalpha_75 <- 160 + 56\nbeta_75 <- 84 - 56 + 53.3\npost_mean_75 <- alpha_75 / (alpha_75 + beta_75)\nlower_credible <- qbeta(.05, 216, 81.3)\nupper_credible <- qbeta(.95, 216, 81.3)\n\npost_75 <- tibble(post_mean_75, lower_credible, upper_credible)\n\npost_75\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  post_mean_75 lower_credible upper_credible\n         <dbl>          <dbl>          <dbl>\n1        0.727          0.683          0.768\n```\n\n\n:::\n:::\n\n\nThe purpose of this project is to attempt to find the probability of Rafael Nadal winning a point on his own serve against Novak Djokovic. To do so, we are creating 3 prior distributions, two of which are based on information from a previous game and a claim made by a sports announcer, and one that is based on no information at all. The first two distributions are called informative distributions, and the other one is a non-informative prior distribution. I will be using a flat prior (Beta(1,1)) for my non-informative prior. After finding the parameters for each prior, I will take the observed data and combine it with the prior distributions to create the posterior distributions.\n\nFor the first informative prior, I divided the points won out of total points served (46/66) to get a mean needed in order to use R code to compute the parameters for the informative priors. The variance was already given, so all that was needed was a target mean, which ended up being the result of 46/66. The resulting parameters were determined to be adequate after \"testing\" them using the beta mean equation to see if the mean matched the target mean, which it did. The assumption that I made when making this prior was that the game that this information was taken from is independent of all other games. I also assumed that the two players had a relatively equal skill level.\n\nFor the second informative prior, I used the 75% as the mean, since the curve would need to be centered at .75 because it's the proportion of points won (according to the sports announcer). To find the parameters, I used code that used a target probability rather than variance since the information given was that Nadal scored no less than 70% of the time which is just P \\>= .7. I decided to increase the value in the sequence function from 100 to 200 because that gave parameters that had a probability equal to .05, which is the target probability that I chose. If I kept the sequence at 100, the code would only produce parameters that had around a .09 probability of no less than 70%. I felt that .09 wasn't a small enough probability because it was too close to .1, so I chose parameters that had a lower probability. I also prefer the parameters from the .05 target probability because with the sequence at 200, the probability is almost exactly .05, compared to when the probability was around .094. For this prior, I also assumed that both players had similar skill levels and were therefore evenly matched when the games were played.\n\nFor the non-informative prior, I chose to do a flat prior using Beta(1,1), since the beta distribution allows you to model a quantity between 0 and 1.\n\nAll of the posterior distributions are different from each other because they all had different parameters and target means due to the difference in information (or lack thereof) provided for each prior distribution. Each posterior distribution has a different mean, so they're centered at different places on the graph. The flat prior had the lowest mean, which makes sense given that the prior distribution was only Beta(1, 1). It also makes sense that the sports announcer's posterior has the highest mean because it began with the highest mean and each distribution shifted the same amount, so the mean would remain the highest.\n\nAll three also saw a decrease in variance compared to their prior distributions, due to an increase in both beta and alpha. The informative posterior distribution that used the sports announcer's information has the lowest variance out of the three curves. This could likely be due to the sports announcer's information being more about Nadal's overall points won percentage against Djokovic compared to the first informative prior that uses data from only one match. Therefore, we are receiving more information from the sports announcer, so the variance is smaller since there's less uncertainty. The posterior distribution from the flat prior has the most variance because there's no information being used. Less variance means that there's a lower variability, meaning that the values are closer together, which could also stem from knowing more information because it helps with accuracy, therefore it would make sense that the values are closer together.\n\nThis certainty is also shown in the credible intervals, where the sports announcer's posterior distribution had the smallest difference between the upper and lower bounds of their 90% credible intervals (a .09 difference) compared to the other informative posterior distribution that had a difference of .119, and the flat prior that had a difference of .167.\n\nIf I had to choose one posterior distribution, I would choose the one that used the previous match for the informative prior because the mean is close to that of the observed data. The variance might be larger than the sports announcer's posterior, but it seems like the observed data would actually fit under the curve, which I think is more important than a lower variance. The credible interval is not wide enough for it to be an issue either.\n\nThe resulting posterior distributions showed that increase both alpha and beta decreased the variance, and a change in mean shifted the distribution curves to the left. The informative posterior distribution that used the most information had the lowest variance and width of its credible interval, but also the highest mean which is likely from the already high target mean used to find the parameters of the prior distribution. The flat prior had the widest variance and smallest mean due to it being non-informative and only having prior distribution of (1,1). The second informative prior that used the previous match saw a decrease in both variance and mean. I think that it's the best one to use because it seems like it includes the observed data under its curve (unlike the other informative posterior) and had a lower variance than the flat distribution.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}