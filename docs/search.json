[
  {
    "objectID": "posts/Reflection/index.html",
    "href": "posts/Reflection/index.html",
    "title": "Reflection",
    "section": "",
    "text": "The first mini-project had us conduct a simulation to investigate an observation made during a recap task where SE(Ymin) is around the same value as SE(Ymax) when n = 5. Through this project, I discovered that this was the case for the normal and uniform distributions given, but not for the exponential and beta distributions. This project relied a lot on graphs, as the symmetry of the points representing the samples on the population graphs helped with coming up with the rule that the project asked me to propose. The graphs helped visualize what was occurring at n = 5 for the given distributions, and provided a way to determine a potential pattern for the distributions that had SE(Ymin) and SE(Ymax) match and a pattern for the distributions whose standard errors weren’t close. In this case, the distributions with close standard errors had symmetrical points on the population graph, while the distributions with different standard errors had asymmetric points under the population curve. The use of graphs in this project ties in with the fourth mini-project, where I compared the prior and posterior distribution curves, finding changes in the mean and variances after updating the prior distributions. These two projects had me practice reading graphs and drawing conclusions based on changes in previous graphs like in the fourth project, or the location of the samples on the population graph like in the first project. Similarly for the third mini-project, instead of using graphs, I had to use the table of collected data and draw conclusions from patterns that I noticed in the average width and coverage rates of different sample sizes and population proportions. For the first mini-project I had also created a table of standard errors and expected values for the corresponding distributions given. The goal of both of these projects was to analyze the differences and similarities between the respective values (so, differences in coverage rates, average widths, and standard errors) and hypothesize why certain differences were closer together or farther apart. With the first project, a rule could be proposed based on the pattern of the points on the population graphs, while the conclusion for the third project was that the small sample size caused a coverage rate that wasn’t equal to the confidence percentage and resulted in a different average width due to the sample size being included in the width equation. This project was one of the first things we did involving R and statistical simulations and the third and fifth mini-projects are a continuation of using statistical simulation. In each project, we used simulation to investigate different things, such as violations in assumptions and the probability of a tennis player scoring a point. Following this project, we spent the rest of the course using statistical simulations in our labs to practice new concepts, analyze data, and answer questions. In the hypothesis testing unit, we did a lab where a researcher wanted to assess whether or not there was evidence that a drug helped lower blood pressure. In that lab, we simulated 10,000 hypothesis tests and recorded each p-value to calculate the empirical power. The null was rejected after finding at least one p-value that was less than alpha. An example that used sampling distributions is lab 3.2, where we simulated 1,000 confidence intervals from a normal distribution with the purpose of finding the coverage rate, which is the fraction of the intervals that contain the parameter from the distribution. We also used statistical simulation to simulate thousands of means from one sample to find the confidence interval of the population mean using bootstrapping.\nMy biggest take-away from this project was that statistical simulation isn’t only for things that require thousands of samples, means, etc. We can also use simulation to investigate something as “simple” as differences and similarities between the standard errors of the sample maximum and minimums of certain distributions. In class, we did a lot of simulations like the bootstrapping lab or the empirical power lab where it was necessary to generate thousands of samples in order to find the empirical power or find the confidence interval of a population based on one sample. This project was unique in that the parameters were known, which made the simulation more simple since I didn’t have to do anything to find the parameters. Though I didn’t have to deal with unknown parameters, it was still difficult to make the graphs using the distribution, as the code was a bit confusing to me. The use of graphs was a major aspect of this project, as they gave useful insight as to why some standard errors had a larger difference than others when comparing the sample maximum and minimums. This project in particular emphasized the importance of analyzing graphs, rather than just relying on the table of results. I further saw this emphasis throughout the remainder of the course, where graphs were not only used to visualize the data, but to also determine what may occur when we complete our lab. The example that most stuck out to me was when we created a power curve in the last lab of the semester. We used the curve to analyze what happened if n increased or decreased. Using the graph, we were able to tell that an increase in n made it so that it was much easier to detect a difference. The use of graphs may be a major aspect of statistical simulation, though they are not the only part. Statistical simulation can also produce different statistics that we are looking for, such as the standard errors of the sample maximums and minimums for this project.\nFor the second mini project, we were required to write a story that conveys the meaning of terms like estimate, likelihood, bias, and variance and demonstrate how they connect with each other. This project required the use of terms that were most used during the estimation unit, though these terms were not specific to that one unit. Terms such as variance, estimate, estimator, parameter, random variable, random sample, and likelihood were used frequently throughout the course, which makes this project the most connected to the rest of the course. This project ties to the other content of the course by ensuring that I understand what the terms I was using actually mean and how they connect to each other. For example, I connected random variable with estimator by having a character in my story explain that random variables are used to create an estimator which in turn is used to create an estimate of an unknown parameter. Following the first unit, we encountered a lot of distributions that had unknown parameters, as it is common that we don’t know at least one parameter in a distribution. Since this story revolved around explaining the process of finding an estimate for an unknown parameter, this ties in with the Bayes unit, where we need to find a prior distribution in order to obtain a posterior distribution, which can then be used to find the Bayes estimate. Throughout the course, we used the majority of the vocabulary that we were required to include in the story. Likelihood was used in the hypothesis testing section to conduct likelihood ratio tests to find which model of the given null and alternative hypotheses fit the best, and when to reject the null hypothesis. Likelihood in the Bayes section was used to help derive the posterior distribution. For this project, I was required to use a specific probability distribution, which we worked with throughout the course. The sampling distribution section served as a review and introduction to this, though the parameters were known to us. However, even though they were known, we still used variance to analyze graphs, like we did in other sections, such as the empirical power lab of the hypothesis testing section where we discussed the impact on delta if the variance was increased or decreased. Due to having to create a story that demonstrates understanding rather than simply defining these terms and explaining their connection to each other, there was a lot of thought that went into figuring out how to write a story that conveys these things. I had to think about how to write a story that included all of these terms, their definitions, and connections in a covert way while also maintaining the flow of the story. I think that the article that I read for the fifth mini project ties in with this aspect, as the authors spent a lot of time discussing how to write interpretations of data in a way that can’t be easily misunderstood due to choices in wording. Both of these projects dealt with thinking about the best ways to convey information to an audience. With this project, it was how to convey my understanding in a non-obvious way (i.e. simply defining the terms); with the fifth mini-project, it was considering the use of the word compatibility in place of significance and confidence. This careful consideration of how to convey my understanding can also tie to the fourth mini project, where I had to justify the way I prior distributions I created instead of just letting the code I used speak for itself. I also had to consider the different posterior distributions and explain which one I would choose, forcing me to display my understanding of the differences in each posterior distribution. If my explanations are not clear, then there is a risk of my justifications for my prior distributions being misunderstood, and if someone reading it were to try to replicate how I produced them, they could likely come up with the wrong distribution. If my reasoning for choosing the posterior distribution were unclear, it could come off as though I don’t understand what makes a good posterior distribution, and that I was just guessing. How to convey my understanding of the topic at hand is one of my biggest take-aways, as it’s very easy to just define the terms and provide one sentence explanations of how they connect. In order to demonstrate true understanding, you should be able to apply these terms in different contexts and use those contexts to demonstrate their meanings and connections. After completing the course, I realized why this particular unit was chosen for this type of project. We see most of the vocabulary words in every section following the first one because there’s always one parameter that is unknown in the distribution, so it’s important to fully understand what these terms mean and how they connect. If we don’t fully understand, or we only understand the surface level meanings and connections akin to the phrase “I know what it means but I can’t define it for you”, it will be much harder to grasp what we are doing in each exercise that uses these terms. For example, if we don’t understand what likelihood means and how it connects to estimation, it would be harder to understand why we were using likelihood in the Bayes unit or the hypothesis testing unit.\nFor the third mini-project, I investigated what happened to confidence intervals if the large sample assumption was violated. It was discovered that the coverage rates would not be equivalent or close together when the population proportion is changed. This meant that when a violation occurred, the coverage rate for a 90% interval would not be close to the required .9 at certain population proportions. This project was a demonstration of the necessity of passing assumptions. Throughout the course, we often assumed that the required assumptions were not violated. In the Bayes unit, we did a lab with women’s hockey data, and it wasn’t until the very last question of the lab that we had actually considered the assumptions we made when conducting the analysis. When we did exercises in class, we mainly focused on assumptions of independence and sample size. Even in some of the labs, we didn’t explicitly check assumptions outside of normality and sample size – we just assumed that the assumptions passed. In the hypothesis testing section, we were more explicit, and checked the assumptions before conducting the proper hypothesis test. This explicit checking of assumptions ties in with the fifth mini-project, where I discussed the concept of statistical thoughtfulness. The author of the article used the questioning of assumptions as an example of statistical thoughtfulness, stating that a statistically thoughtful researcher would question whether or not the study passed the proper assumptions. This mini-project demonstrates the importance of ensuring that the assumptions pass, as a consequence of a failed large sample assumption is that the confidence intervals won’t have the correct amount of coverage rate, rendering the confidence interval a bad one. If the confidence interval isn’t good, it won’t provide the proper interval, which would lead to a poor analysis. The article in the fifth mini-project expressed a lot of concern about people misunderstanding results of studies, particularly due to the wording of interpretations, one of those interpretations involving confidence intervals. I think that this is also a connection to this mini-project, because a confidence interval produced from a too small sample size would create a false understanding of the intended subject of analysis. The biggest take-away of this mini-project is the importance of ensuring that assumptions are not violated, as the results it produces will be incorrect and could lead to misinformation. If the assumption is violated, it could result in an error when conducting analyses or using equations to find something like the test-statistic, which uses the sample size. When an error occurs in any aspect of an equation or a process such as the likelihood ratio test, it can become impossible to actually solve or complete the equation or test, or will produce an incorrect result that can affect the rest of the analysis. For example, if an error occurs during the likelihood step of the likelihood ratio test, it becomes impossible to derive the test or come to the correct result. In certain circumstances the violation of the large sample assumption is made known through the coverage rate and average width of confidence intervals or the test-statistic equation. In other circumstances, the violations are not as obvious unless you are able to identify that something is wrong with your result. This circumstance is particularly troublesome, because there’s a likelihood that the error isn’t caught, allowing the statistician to accidentally present incorrect information.\nFor the fourth mini-project, I had to create non-informative prior distributions and informative prior distributions based on information provided about tennis player Nadal’s winning points. I then used updated information to create posterior distributions for the purpose of finding the probability that Nadal wins a point. The reason for creating these prior and posterior distributions was that there were no known parameters. Though this project uses Bayesian statistics instead of frequentist statistics, unknown parameters were a regular occurrence after the first section. Through this, this project ties in with the work we did with unknown parameters. In this project, I used prior information to create a probability distribution that captures the information. In the estimation section, we used methods of moments and maximum likelihood estimation to estimate unknown parameters. We also saw unknown parameters in the confidence interval section, where we used bootstrapping in order to find the confidence interval of an unknown population parameter and the pivot method to find the confidence interval of an unknown parameter. In the hypothesis section, we derived the most powerful test using a given distribution with a missing parameter, although we were given values for the null and alternative hypotheses. This project provided another method of dealing with unknown parameters using a different statistical philosophy, as all of the other cases we encountered used frequentist methods. Instead of using likelihood functions or pivoting for this project, I used statistical simulation to find the alpha and beta of both prior and posterior distributions. I also used statistical simulation in the first mini-project to find and compare the standard errors of the sample maximums and minimums. Like that project, I also used graphs to analyze the data, finding indications of changes in variance and mean after updating the prior distributions using information from the 2020 French Open data. Another thing that I had to do for this mini-project was justify my prior distributions, which ties in with the fifth mini-project, where I was asked what I thought statistical thoughtfulness meant. With context from the article, I mentioned that it involved justifying everything that is done during a study or experiment. In this case, I justified the decisions I made when I created the informative priors, opting to use the given fraction (46/66) and 75% as the respective inputted means in the simulation I used to produce the alphas and betas. This was an exercise in statistical thoughtfulness, as not only it required me to really think about what I was actually doing and why. My biggest takeaway from this project is that even if you don’t know the parameters, you can still make a distribution. In frequentist statistics we saw the use of the maximum likelihood to find an estimate of an unknown parameter. In Bayesian statistics and this project, you can make prior distributions and update them when more information arises. Like in frequentist statistics, some posterior distributions (or estimates, in frequentist statistics) are going to be better than others. The more information you have, the better the resulting prior and posterior distributions. In this project, however, when I was choosing what posterior distribution to use, I had to decide between the lower variance or a mean that was closer to the observed mean. Through this exercise, I learned that there are going to be some situations where you need to decide between two different desirable statistics, such as mean or variance, when choosing the best distribution. Similarly, this is seen in frequentist statistics with bias-variance trade-off when determining the best estimate.\nFor the last mini-project, I had to read an article titled Moving to a World Beyond p &lt; .05. This mini-project had me consider what the author meant when they brought up the terms “statistical thinking” and “statistical thoughtfulness”. Both of these terms apply to the field of statistics as a whole, as a good statistician is statistically thoughtful. Throughout this class, we employed statistical thinking by considering things like the best prior distribution to use in the Bayes unit, as well as questioning what assumptions were made during the process of finding a posterior distribution. During the confidence interval unit, we also employed statistical thoughtfulness when choosing the proper pivot quantity and justifying our choice through the rules required for a correct pivot quantity. Additionally, there was a question about the reading pertaining to the article’s author considering using the word compatibility in place of the word confident when interpreting confidence intervals. This part also ties into the confidence interval unit, as I had to consider the possibility of my interpretations being misunderstood due to people taking the use of the word confident to mean certain and correct. Similarly, this can be applied to the Bayes unit, where we interpreted credible intervals. The difference in interpretations of credible and confidence intervals was addressed in class, and we discussed how the interpretations for credible intervals were easier to understand since we use the term “probability” instead of “confidence”. Overall, this project had me considering the way that I analyzed and interpreted data. It put emphasis on choice of wording, particularly the use of “statistically significant” and “confident”. The authors wanted to minimize misunderstandings and misinterpretations as much as possible. I think that this aspect of the article, along with the questions associated with these considerations tie in with the second mini-project, where I was tasked with writing a meaningful story that not only demonstrated my understanding of specific terms, but also how these terms connected with each other. As a result, I had to put a lot of thought into how to use the required terms in a way readers could understand how they connected. Both projects required thought about conveying information and minimizing misunderstanding. Though the article talked a lot about word choices, it also discussed de-emphasizing the use of p-values to evaluate a result and whether or not to share the result in published research. There were a couple of related questions pertaining to statistical inference and how to determine which results to present in research. The conclusion that I came to was that there was no one-size fits all approach to either question. In the hypothesis testing section for example, we used the likelihood ratio test to compare models using the null and alternative hypotheses given. When it comes to choosing which results to present, I responded to the corresponding question in this mini-project with a suggestion that including mistakes is just as valuable as including desired results as they can reveal new information about what’s being studied. The third mini-project is a demonstration of this, as we investigated what occurs when the large sample assumption is violated. Through this project, we saw the impact that the violation had on the average width and coverage rate of a confidence interval. Due to this violation (and therefore mistake) we were able to see how the sample size impacted the width and coverage rate. Even though the assumptions were violated, we were still able to learn something about confidence intervals, which is just as important as discussing the results that may have occurred if the large sample assumption passed. \nMy biggest take-away from this project is that you don’t have to be perfect in the studies you conduct, as there are still plenty of things to be learned from mistakes that are made. We saw this in the third mini-project, where we observed the effects of a violated large sample assumption. In class, we often discussed mistakes that were made and even if it wasn’t a mistake that caused a significant change, it was still a learning lesson, as we learned about some common mistakes that could be made when solving problems. For example, when we were deriving the most powerful test, we learned that making a mistake in the algebra or likelihood step makes it impossible to derive the correct result. Another thing I learned from this project was that statistical thoughtfulness comes in many shapes and forms. When we were testing the bias and consistency of our estimates in the estimation section, we were practicing statistical thoughtfulness. When we were writing the interpretations of our confidence intervals, credible intervals, and hypothesis tests, we were practicing statistical thoughtfulness. Each project that we had this semester was an exercise in different forms of statistical thoughtfulness, be it an exploration of assumption violation, conveying the connections and meanings of specific terms, further investigating an observation made in class, justifying the creation of prior distributions using given information, or considering which results to present."
  },
  {
    "objectID": "posts/04-bayesian/index.html",
    "href": "posts/04-bayesian/index.html",
    "title": "Project 4: Bayesian Statistics",
    "section": "",
    "text": "library(tidyverse)\n\n\n# second\ntarget_mean &lt;- .7\n\nalphas &lt;- seq(0.1, 66, length.out = 500)\nbetas &lt;- .3 * alphas / .7\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\n\ntarget_var &lt;- .05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.3  19.4 0.00320     0.00000214\n\n\n\ntarget_mean &lt;- .75\n\nalphas &lt;- seq(0.1, 200, length.out = 2000)\nbetas &lt;- .25 * alphas / .75\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2 * (alphas + betas + 1)))\n\ntarget_prob &lt;- .05\nprob_less_.7 &lt;- pbeta(.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_.7) |&gt;\n  mutate(close_to_target = abs(prob_less_.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1   160.  53.3       0.0500      0.00000851\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha &lt;- 45.3 \ninformative_beta &lt;- 19.4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_priorfrac &lt;- dbeta(ps, informative_alpha,\n                           informative_beta)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_2 &lt;- 160\ninformative_beta_2 &lt;- 53.3\ninformative_prior70 &lt;- dbeta(ps, informative_alpha_2,\n                          informative_beta_2)\n\nnoninformative_alpha_1 &lt;- 1\nnoninformative_beta_1 &lt;- 1\nnoninformative_1 &lt;- dbeta(ps, noninformative_alpha_1,\n                             noninformative_beta_1)\n\nplot_df &lt;- tibble(ps, informative_priorfrac, noninformative_prior,\n                     informative_prior70) |&gt;\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Prior Distributions\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha_post1 &lt;- 45.3 + 56 \ninformative_beta_post1 &lt;-  84 - 56 + 19.4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_postfrac &lt;- dbeta(ps, informative_alpha_post1,\n                           informative_beta_post1)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\ninformative_alpha_post70 &lt;- 160 + 56\ninformative_beta_post70 &lt;- 84 - 56 + 53.3\ninformative_post70 &lt;- dbeta(ps, informative_alpha_post70,\n                          informative_beta_post70)\n\nnoninformative_alpha_postf &lt;- 56 + 1\nnoninformative_beta_postf &lt;-  84 - 56 + 1\nnoninformative_postflat &lt;- dbeta(ps, noninformative_alpha_postf,\n                             noninformative_beta_postf)\n\nplot_df &lt;- tibble(ps, informative_postfrac,\n                     informative_post70, noninformative_postflat) |&gt;\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"))\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = prior_type,\n                           linetype = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Posterior Distributions\")\n\n\n\n\n\n\n\n\n\n# flat posterior\nalpha_flat &lt;- 56 + 1\nbeta_flat &lt;- 84 - 56 + 1\npost_mean_flat &lt;- alpha_flat / (alpha_flat + beta_flat)\nlower_bound_credible_flat &lt;- qbeta(.05, 57, 29)\nupper_bound_crebile_flat &lt;- qbeta(.95, 57, 29)\n\nflat &lt;- tibble(post_mean_flat, lower_bound_credible_flat, upper_bound_crebile_flat)\nflat\n\n# A tibble: 1 × 3\n  post_mean_flat lower_bound_credible_flat upper_bound_crebile_flat\n           &lt;dbl&gt;                     &lt;dbl&gt;                    &lt;dbl&gt;\n1          0.663                     0.577                    0.744\n\n# 46 out of 66 informative\nalpha_46 &lt;- 45.3 + 56\nbeta_46 &lt;- 84 - 56 + 19.4\npost_mean_46 &lt;- alpha_46 / (alpha_46 + beta_46)\nlower_bound_credible &lt;- qbeta(.05, 101.3, 88)\nupper_bound_credible &lt;- qbeta(.95, 101.3, 88)\n\npost_46 &lt;- tibble(post_mean_46, lower_bound_credible, upper_bound_credible)\n\npost_46\n\n# A tibble: 1 × 3\n  post_mean_46 lower_bound_credible upper_bound_credible\n         &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1        0.681                0.475                0.594\n\n# 75% prior\nalpha_75 &lt;- 160 + 56\nbeta_75 &lt;- 84 - 56 + 53.3\npost_mean_75 &lt;- alpha_75 / (alpha_75 + beta_75)\nlower_credible &lt;- qbeta(.05, 216, 81.3)\nupper_credible &lt;- qbeta(.95, 216, 81.3)\n\npost_75 &lt;- tibble(post_mean_75, lower_credible, upper_credible)\n\npost_75\n\n# A tibble: 1 × 3\n  post_mean_75 lower_credible upper_credible\n         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1        0.727          0.683          0.768\n\n\nThe purpose of this project is to attempt to find the probability of Rafael Nadal winning a point on his own serve against Novak Djokovic. To do so, we are creating 3 prior distributions, two of which are based on information from a previous game and a claim made by a sports announcer, and one that is based on no information at all. The first two distributions are called informative distributions, and the other one is a non-informative prior distribution. I will be using a flat prior (Beta(1,1)) for my non-informative prior. After finding the parameters for each prior, I will take the observed data and combine it with the prior distributions to create the posterior distributions.\nFor the first informative prior, I divided the points won out of total points served (46/66) to get a mean needed in order to use R code to compute the parameters for the informative priors. The variance was already given, so all that was needed was a target mean, which ended up being the result of 46/66. The resulting parameters were determined to be adequate after “testing” them using the beta mean equation to see if the mean matched the target mean, which it did. The assumption that I made when making this prior was that the game that this information was taken from is independent of all other games. I also assumed that the two players had a relatively equal skill level.\nFor the second informative prior, I used the 75% as the mean, since the curve would need to be centered at .75 because it’s the proportion of points won (according to the sports announcer). To find the parameters, I used code that used a target probability rather than variance since the information given was that Nadal scored no less than 70% of the time which is just P &gt;= .7. I decided to increase the value in the sequence function from 100 to 200 because that gave parameters that had a probability equal to .05, which is the target probability that I chose. If I kept the sequence at 100, the code would only produce parameters that had around a .09 probability of no less than 70%. I felt that .09 wasn’t a small enough probability because it was too close to .1, so I chose parameters that had a lower probability. I also prefer the parameters from the .05 target probability because with the sequence at 200, the probability is almost exactly .05, compared to when the probability was around .094. For this prior, I also assumed that both players had similar skill levels and were therefore evenly matched when the games were played.\nFor the non-informative prior, I chose to do a flat prior using Beta(1,1), since the beta distribution allows you to model a quantity between 0 and 1.\nAll of the posterior distributions are different from each other because they all had different parameters and target means due to the difference in information (or lack thereof) provided for each prior distribution. Each posterior distribution has a different mean, so they’re centered at different places on the graph. The flat prior had the lowest mean, which makes sense given that the prior distribution was only Beta(1, 1). It also makes sense that the sports announcer’s posterior has the highest mean because it began with the highest mean and each distribution shifted the same amount, so the mean would remain the highest.\nAll three also saw a decrease in variance compared to their prior distributions, due to an increase in both beta and alpha. The informative posterior distribution that used the sports announcer’s information has the lowest variance out of the three curves. This could likely be due to the sports announcer’s information being more about Nadal’s overall points won percentage against Djokovic compared to the first informative prior that uses data from only one match. Therefore, we are receiving more information from the sports announcer, so the variance is smaller since there’s less uncertainty. The posterior distribution from the flat prior has the most variance because there’s no information being used. Less variance means that there’s a lower variability, meaning that the values are closer together, which could also stem from knowing more information because it helps with accuracy, therefore it would make sense that the values are closer together.\nThis certainty is also shown in the credible intervals, where the sports announcer’s posterior distribution had the smallest difference between the upper and lower bounds of their 90% credible intervals (a .09 difference) compared to the other informative posterior distribution that had a difference of .119, and the flat prior that had a difference of .167.\nIf I had to choose one posterior distribution, I would choose the one that used the previous match for the informative prior because the mean is close to that of the observed data. The variance might be larger than the sports announcer’s posterior, but it seems like the observed data would actually fit under the curve, which I think is more important than a lower variance. The credible interval is not wide enough for it to be an issue either.\nThe resulting posterior distributions showed that increase both alpha and beta decreased the variance, and a change in mean shifted the distribution curves to the left. The informative posterior distribution that used the most information had the lowest variance and width of its credible interval, but also the highest mean which is likely from the already high target mean used to find the parameters of the prior distribution. The flat prior had the widest variance and smallest mean due to it being non-informative and only having prior distribution of (1,1). The second informative prior that used the previous match saw a decrease in both variance and mean. I think that it’s the best one to use because it seems like it includes the observed data under its curve (unlike the other informative posterior) and had a lower variance than the flat distribution."
  },
  {
    "objectID": "posts/02-estimation/index.html",
    "href": "posts/02-estimation/index.html",
    "title": "Project 2: Estimation",
    "section": "",
    "text": "The normal distribution family is nearly complete. They have their parameter, standard deviation, which they received from the upper beings upon the creation of the normal model, and is all that they could ever ask for. However, they always felt like they were missing something. They couldn’t understand what it was that they were missing, as every time they tried to get to the bottom of it, their minds went blank. Bothered by this confusion, they consulted a magic deity, who revealed the problem to them. It turns out that the normal distribution family was missing an important parameter, the mean! The family was unable to determine what it was that they were missing because the mean was unknown to them. The value assigned to a being registers them to a family, but since they didn’t receive a mean with a value, its existence was forgotten, while the standard deviation was remembered because they knew its value. Distraught at this revelation, the family asked the deity what they could do to find their unknown mean. The deity explained to them that since the mean was not assigned a value immediately upon introduction to the family, they’ll have to find the best estimate for the unknown parameter. It likely won’t be the exact correct value, but if done correctly, the estimate should be good enough. The family was unsure how to do this, so the deity summoned a magic hand to help them.\nThe hand was at least 10 feet tall and had a purple sack around the stub that protruded from the end of its palm. The family and the giant limb exchanged pleasantries before the giant hand dropped the purple sack to the ground. The sack opened upon hitting the ground and an innumerable amount of gems with letters carved into them were revealed. The sack was never-ending, and the family could only make out some of the letters. They were able to see X1, X5, and Xn from where they were standing. The hand explained that the sack of gems represented identically and independently distributed random variables which represent a random sample from the population of the normal model that led to the existence of the family and its standard deviation. The hand explained that the random variable was going to be used to create an estimator needed to help find the estimate. The giant hand then emitted a vibrating sound and the ground shook, causing the sack to explode, leaving a wand in its place. The giant hand lifted the magic wand, introducing it as the Estimator31400. On the wand was a tiny number pad, which the giant hand explained was for changing the values of the function that powers the wand. One of those values pertained to the sample size. The family looked at the wand in awe, asking the giant hand what had just happened. The giant hand explained that it sacrificed the sack of gems to the deities of likelihood. The deities then gifted them the wand in return, having carved it out of one of their great logs. The giant hand went on to explain that when used, the wand derives the maximum likelihood estimator of the unknown mean by taking the random variables (the sack) and applying the magical power of the deity’s great log. The wand took the gems, which had numbers on the back of them that represented observed values from the random sample and maximized the function from the deities of likelihood so that the observed values were the most probable given the estimated parameter. It further explained that this was only one way that they could find an estimate for their missing parameter.\nThe family, still awestruck by this magic wand, asked the giant hand how to use it. The hand showed them that they could use the number pad to change the values of the given function to produce an estimate. As the family plugged the proper numbers in, the wand produced a ball of light. Within the light appeared a number: the estimate. The family felt a rush of excitement, finally able to see an estimate for the mean. The hand explained that the family ought to ensure that the estimator is a good estimator.\nThe air around the family picked up, and a huge gust of wind swept past them, momentarily disorienting them. When they regained their senses, another wand was in front of them, producing its own ball of light and number. The giant hand described this one as the method of moments estimator, which was produced by the deities of momentous. The giant hand explained that it was preferable to have two different estimators to compare, that way the family had more of a choice in how they wanted to find the estimate. In order to compare the two, the estimators had to go through a series of trials. The estimates made within the balls of light disappeared as the balls became blank slates, representing their respective estimators.\nFirst was a test of bias. The goal of this trial is for the estimator to be considered unbiased rather than biased. If an estimator is biased, then the estimate produced would be poisoned and no good for the family. The family looked nervous about the prospect of a bad and poisoned estimate but the giant hand was quick to reassure them that they can make adjustments and find an unbiased estimator and that the estimator can be biased under some circumstances but an unbiased estimator is preferred for the sake of this trial.. First, what occurred was that the “great expectations” of both estimators were found.. The results were put up against the great mewling mu which represented the population parameter, and anything but a result of zero was to be marked for bias. Luckily for the family, both estimators were accepted as unbiased, as it appeared that the great expectations matched the mewling mu.\nThe second trial was a test of variance. Here, the family needed to find the balls of light as they shrunk to match the size of their respective variances and then line them up in a tray. The balls of light exploded into many balls of the same size.The family scrambled after the balls as they rolled around on the floor, only catching them after a few moments. It appeared to the family that the smaller balls of light came from the maximum likelihood estimator. When the family lined up the balls, they found that the smaller ones were closer together, as their size required less distance apart, while the larger balls of light were farther apart and thus a smaller amount of balls were able to fit inside the tray compared to the smaller balls, making the smaller balls the winner of this trial. The variance came with numerical values of course, and when the two were divided in order to find their relative efficiency, the giant hand found that the result came out larger than one, resulting in the maximum likelihood estimator being deemed more efficient as well.\nThe third trial was where variance and bias became entangled with one another, as the balls representing the variance and the biased values of their respective estimators combined with each other to create two giant bright lights. The two lights dimmed as the new values in newly created balls of light emerged, one smaller than the other. The smaller one, belonging to the maximum likelihood estimator, fit snugly in the palm of a family member’s hand, while the larger one was too big for comfort, resulting in the smaller one becoming the favored one.\nThe final trial required the use of the wands, as the family was instructed to change the sample size and observe the change in size of the balls of light. The giant hand wanted to test the consistency of each of the estimators to see how close to the mewling mu they could get. The family observed that as the sample size increased, both of the estimator’s variances became smaller. The giant hand told the family that the decrease in size was a good sign, as the estimators seem to be consistent and therefore close to the mewling mu.\nAt the end of the trials, the family decided that the maximum likelihood estimator is the best estimator of the two, and with a few adjustments to the wand, their hard work is rewarded with their new estimate for their no longer unknown parameter."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog_326",
    "section": "",
    "text": "Project 1: Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2: Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 3: Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 4: Bayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 5: Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Reflection",
    "section": "",
    "text": "The first mini-project had us conduct a simulation to investigate an observation made during a recap task where SE(Ymin) is around the same value as SE(Ymax) when n = 5. Through this project, I discovered that this was the case for the normal and uniform distributions given, but not for the exponential and beta distributions. This project relied a lot on graphs, as the symmetry of the points representing the samples on the population graphs helped with coming up with the rule that the project asked me to propose. The graphs helped visualize what was occurring at n = 5 for the given distributions, and provided a way to determine a potential pattern for the distributions that had SE(Ymin) and SE(Ymax) match and a pattern for the distributions whose standard errors weren’t close. In this case, the distributions with close standard errors had symmetrical points on the population graph, while the distributions with different standard errors had asymmetric points under the population curve. The use of graphs in this project ties in with the fourth mini-project, where I compared the prior and posterior distribution curves, finding changes in the mean and variances after updating the prior distributions. These two projects had me practice reading graphs and drawing conclusions based on changes in previous graphs like in the fourth project, or the location of the samples on the population graph like in the first project. Similarly for the third mini-project, instead of using graphs, I had to use the table of collected data and draw conclusions from patterns that I noticed in the average width and coverage rates of different sample sizes and population proportions. For the first mini-project I had also created a table of standard errors and expected values for the corresponding distributions given. The goal of both of these projects was to analyze the differences and similarities between the respective values (so, differences in coverage rates, average widths, and standard errors) and hypothesize why certain differences were closer together or farther apart. With the first project, a rule could be proposed based on the pattern of the points on the population graphs, while the conclusion for the third project was that the small sample size caused a coverage rate that wasn’t equal to the confidence percentage and resulted in a different average width due to the sample size being included in the width equation. This project was one of the first things we did involving R and statistical simulations and the third and fifth mini-projects are a continuation of using statistical simulation. In each project, we used simulation to investigate different things, such as violations in assumptions and the probability of a tennis player scoring a point. Following this project, we spent the rest of the course using statistical simulations in our labs to practice new concepts, analyze data, and answer questions. In the hypothesis testing unit, we did a lab where a researcher wanted to assess whether or not there was evidence that a drug helped lower blood pressure. In that lab, we simulated 10,000 hypothesis tests and recorded each p-value to calculate the empirical power. The null was rejected after finding at least one p-value that was less than alpha. An example that used sampling distributions is lab 3.2, where we simulated 1,000 confidence intervals from a normal distribution with the purpose of finding the coverage rate, which is the fraction of the intervals that contain the parameter from the distribution. We also used statistical simulation to simulate thousands of means from one sample to find the confidence interval of the population mean using bootstrapping.\nMy biggest take-away from this project was that statistical simulation isn’t only for things that require thousands of samples, means, etc. We can also use simulation to investigate something as “simple” as differences and similarities between the standard errors of the sample maximum and minimums of certain distributions. In class, we did a lot of simulations like the bootstrapping lab or the empirical power lab where it was necessary to generate thousands of samples in order to find the empirical power or find the confidence interval of a population based on one sample. This project was unique in that the parameters were known, which made the simulation more simple since I didn’t have to do anything to find the parameters. Though I didn’t have to deal with unknown parameters, it was still difficult to make the graphs using the distribution, as the code was a bit confusing to me. The use of graphs was a major aspect of this project, as they gave useful insight as to why some standard errors had a larger difference than others when comparing the sample maximum and minimums. This project in particular emphasized the importance of analyzing graphs, rather than just relying on the table of results. I further saw this emphasis throughout the remainder of the course, where graphs were not only used to visualize the data, but to also determine what may occur when we complete our lab. The example that most stuck out to me was when we created a power curve in the last lab of the semester. We used the curve to analyze what happened if n increased or decreased. Using the graph, we were able to tell that an increase in n made it so that it was much easier to detect a difference. The use of graphs may be a major aspect of statistical simulation, though they are not the only part. Statistical simulation can also produce different statistics that we are looking for, such as the standard errors of the sample maximums and minimums for this project.\nFor the second mini project, we were required to write a story that conveys the meaning of terms like estimate, likelihood, bias, and variance and demonstrate how they connect with each other. This project required the use of terms that were most used during the estimation unit, though these terms were not specific to that one unit. Terms such as variance, estimate, estimator, parameter, random variable, random sample, and likelihood were used frequently throughout the course, which makes this project the most connected to the rest of the course. This project ties to the other content of the course by ensuring that I understand what the terms I was using actually mean and how they connect to each other. For example, I connected random variable with estimator by having a character in my story explain that random variables are used to create an estimator which in turn is used to create an estimate of an unknown parameter. Following the first unit, we encountered a lot of distributions that had unknown parameters, as it is common that we don’t know at least one parameter in a distribution. Since this story revolved around explaining the process of finding an estimate for an unknown parameter, this ties in with the Bayes unit, where we need to find a prior distribution in order to obtain a posterior distribution, which can then be used to find the Bayes estimate. Throughout the course, we used the majority of the vocabulary that we were required to include in the story. Likelihood was used in the hypothesis testing section to conduct likelihood ratio tests to find which model of the given null and alternative hypotheses fit the best, and when to reject the null hypothesis. Likelihood in the Bayes section was used to help derive the posterior distribution. For this project, I was required to use a specific probability distribution, which we worked with throughout the course. The sampling distribution section served as a review and introduction to this, though the parameters were known to us. However, even though they were known, we still used variance to analyze graphs, like we did in other sections, such as the empirical power lab of the hypothesis testing section where we discussed the impact on delta if the variance was increased or decreased. Due to having to create a story that demonstrates understanding rather than simply defining these terms and explaining their connection to each other, there was a lot of thought that went into figuring out how to write a story that conveys these things. I had to think about how to write a story that included all of these terms, their definitions, and connections in a covert way while also maintaining the flow of the story. I think that the article that I read for the fifth mini project ties in with this aspect, as the authors spent a lot of time discussing how to write interpretations of data in a way that can’t be easily misunderstood due to choices in wording. Both of these projects dealt with thinking about the best ways to convey information to an audience. With this project, it was how to convey my understanding in a non-obvious way (i.e. simply defining the terms); with the fifth mini-project, it was considering the use of the word compatibility in place of significance and confidence. This careful consideration of how to convey my understanding can also tie to the fourth mini project, where I had to justify the way I prior distributions I created instead of just letting the code I used speak for itself. I also had to consider the different posterior distributions and explain which one I would choose, forcing me to display my understanding of the differences in each posterior distribution. If my explanations are not clear, then there is a risk of my justifications for my prior distributions being misunderstood, and if someone reading it were to try to replicate how I produced them, they could likely come up with the wrong distribution. If my reasoning for choosing the posterior distribution were unclear, it could come off as though I don’t understand what makes a good posterior distribution, and that I was just guessing. How to convey my understanding of the topic at hand is one of my biggest take-aways, as it’s very easy to just define the terms and provide one sentence explanations of how they connect. In order to demonstrate true understanding, you should be able to apply these terms in different contexts and use those contexts to demonstrate their meanings and connections. After completing the course, I realized why this particular unit was chosen for this type of project. We see most of the vocabulary words in every section following the first one because there’s always one parameter that is unknown in the distribution, so it’s important to fully understand what these terms mean and how they connect. If we don’t fully understand, or we only understand the surface level meanings and connections akin to the phrase “I know what it means but I can’t define it for you”, it will be much harder to grasp what we are doing in each exercise that uses these terms. For example, if we don’t understand what likelihood means and how it connects to estimation, it would be harder to understand why we were using likelihood in the Bayes unit or the hypothesis testing unit.\nFor the third mini-project, I investigated what happened to confidence intervals if the large sample assumption was violated. It was discovered that the coverage rates would not be equivalent or close together when the population proportion is changed. This meant that when a violation occurred, the coverage rate for a 90% interval would not be close to the required .9 at certain population proportions. This project was a demonstration of the necessity of passing assumptions. Throughout the course, we often assumed that the required assumptions were not violated. In the Bayes unit, we did a lab with women’s hockey data, and it wasn’t until the very last question of the lab that we had actually considered the assumptions we made when conducting the analysis. When we did exercises in class, we mainly focused on assumptions of independence and sample size. Even in some of the labs, we didn’t explicitly check assumptions outside of normality and sample size – we just assumed that the assumptions passed. In the hypothesis testing section, we were more explicit, and checked the assumptions before conducting the proper hypothesis test. This explicit checking of assumptions ties in with the fifth mini-project, where I discussed the concept of statistical thoughtfulness. The author of the article used the questioning of assumptions as an example of statistical thoughtfulness, stating that a statistically thoughtful researcher would question whether or not the study passed the proper assumptions. This mini-project demonstrates the importance of ensuring that the assumptions pass, as a consequence of a failed large sample assumption is that the confidence intervals won’t have the correct amount of coverage rate, rendering the confidence interval a bad one. If the confidence interval isn’t good, it won’t provide the proper interval, which would lead to a poor analysis. The article in the fifth mini-project expressed a lot of concern about people misunderstanding results of studies, particularly due to the wording of interpretations, one of those interpretations involving confidence intervals. I think that this is also a connection to this mini-project, because a confidence interval produced from a too small sample size would create a false understanding of the intended subject of analysis. The biggest take-away of this mini-project is the importance of ensuring that assumptions are not violated, as the results it produces will be incorrect and could lead to misinformation. If the assumption is violated, it could result in an error when conducting analyses or using equations to find something like the test-statistic, which uses the sample size. When an error occurs in any aspect of an equation or a process such as the likelihood ratio test, it can become impossible to actually solve or complete the equation or test, or will produce an incorrect result that can affect the rest of the analysis. For example, if an error occurs during the likelihood step of the likelihood ratio test, it becomes impossible to derive the test or come to the correct result. In certain circumstances the violation of the large sample assumption is made known through the coverage rate and average width of confidence intervals or the test-statistic equation. In other circumstances, the violations are not as obvious unless you are able to identify that something is wrong with your result. This circumstance is particularly troublesome, because there’s a likelihood that the error isn’t caught, allowing the statistician to accidentally present incorrect information.\nFor the fourth mini-project, I had to create non-informative prior distributions and informative prior distributions based on information provided about tennis player Nadal’s winning points. I then used updated information to create posterior distributions for the purpose of finding the probability that Nadal wins a point. The reason for creating these prior and posterior distributions was that there were no known parameters. Though this project uses Bayesian statistics instead of frequentist statistics, unknown parameters were a regular occurrence after the first section. Through this, this project ties in with the work we did with unknown parameters. In this project, I used prior information to create a probability distribution that captures the information. In the estimation section, we used methods of moments and maximum likelihood estimation to estimate unknown parameters. We also saw unknown parameters in the confidence interval section, where we used bootstrapping in order to find the confidence interval of an unknown population parameter and the pivot method to find the confidence interval of an unknown parameter. In the hypothesis section, we derived the most powerful test using a given distribution with a missing parameter, although we were given values for the null and alternative hypotheses. This project provided another method of dealing with unknown parameters using a different statistical philosophy, as all of the other cases we encountered used frequentist methods. Instead of using likelihood functions or pivoting for this project, I used statistical simulation to find the alpha and beta of both prior and posterior distributions. I also used statistical simulation in the first mini-project to find and compare the standard errors of the sample maximums and minimums. Like that project, I also used graphs to analyze the data, finding indications of changes in variance and mean after updating the prior distributions using information from the 2020 French Open data. Another thing that I had to do for this mini-project was justify my prior distributions, which ties in with the fifth mini-project, where I was asked what I thought statistical thoughtfulness meant. With context from the article, I mentioned that it involved justifying everything that is done during a study or experiment. In this case, I justified the decisions I made when I created the informative priors, opting to use the given fraction (46/66) and 75% as the respective inputted means in the simulation I used to produce the alphas and betas. This was an exercise in statistical thoughtfulness, as not only it required me to really think about what I was actually doing and why. My biggest takeaway from this project is that even if you don’t know the parameters, you can still make a distribution. In frequentist statistics we saw the use of the maximum likelihood to find an estimate of an unknown parameter. In Bayesian statistics and this project, you can make prior distributions and update them when more information arises. Like in frequentist statistics, some posterior distributions (or estimates, in frequentist statistics) are going to be better than others. The more information you have, the better the resulting prior and posterior distributions. In this project, however, when I was choosing what posterior distribution to use, I had to decide between the lower variance or a mean that was closer to the observed mean. Through this exercise, I learned that there are going to be some situations where you need to decide between two different desirable statistics, such as mean or variance, when choosing the best distribution. Similarly, this is seen in frequentist statistics with bias-variance trade-off when determining the best estimate.\nFor the last mini-project, I had to read an article titled Moving to a World Beyond p &lt; .05. This mini-project had me consider what the author meant when they brought up the terms “statistical thinking” and “statistical thoughtfulness”. Both of these terms apply to the field of statistics as a whole, as a good statistician is statistically thoughtful. Throughout this class, we employed statistical thinking by considering things like the best prior distribution to use in the Bayes unit, as well as questioning what assumptions were made during the process of finding a posterior distribution. During the confidence interval unit, we also employed statistical thoughtfulness when choosing the proper pivot quantity and justifying our choice through the rules required for a correct pivot quantity. Additionally, there was a question about the reading pertaining to the article’s author considering using the word compatibility in place of the word confident when interpreting confidence intervals. This part also ties into the confidence interval unit, as I had to consider the possibility of my interpretations being misunderstood due to people taking the use of the word confident to mean certain and correct. Similarly, this can be applied to the Bayes unit, where we interpreted credible intervals. The difference in interpretations of credible and confidence intervals was addressed in class, and we discussed how the interpretations for credible intervals were easier to understand since we use the term “probability” instead of “confidence”. Overall, this project had me considering the way that I analyzed and interpreted data. It put emphasis on choice of wording, particularly the use of “statistically significant” and “confident”. The authors wanted to minimize misunderstandings and misinterpretations as much as possible. I think that this aspect of the article, along with the questions associated with these considerations tie in with the second mini-project, where I was tasked with writing a meaningful story that not only demonstrated my understanding of specific terms, but also how these terms connected with each other. As a result, I had to put a lot of thought into how to use the required terms in a way readers could understand how they connected. Both projects required thought about conveying information and minimizing misunderstanding. Though the article talked a lot about word choices, it also discussed de-emphasizing the use of p-values to evaluate a result and whether or not to share the result in published research. There were a couple of related questions pertaining to statistical inference and how to determine which results to present in research. The conclusion that I came to was that there was no one-size fits all approach to either question. In the hypothesis testing section for example, we used the likelihood ratio test to compare models using the null and alternative hypotheses given. When it comes to choosing which results to present, I responded to the corresponding question in this mini-project with a suggestion that including mistakes is just as valuable as including desired results as they can reveal new information about what’s being studied. The third mini-project is a demonstration of this, as we investigated what occurs when the large sample assumption is violated. Through this project, we saw the impact that the violation had on the average width and coverage rate of a confidence interval. Due to this violation (and therefore mistake) we were able to see how the sample size impacted the width and coverage rate. Even though the assumptions were violated, we were still able to learn something about confidence intervals, which is just as important as discussing the results that may have occurred if the large sample assumption passed. \nMy biggest take-away from this project is that you don’t have to be perfect in the studies you conduct, as there are still plenty of things to be learned from mistakes that are made. We saw this in the third mini-project, where we observed the effects of a violated large sample assumption. In class, we often discussed mistakes that were made and even if it wasn’t a mistake that caused a significant change, it was still a learning lesson, as we learned about some common mistakes that could be made when solving problems. For example, when we were deriving the most powerful test, we learned that making a mistake in the algebra or likelihood step makes it impossible to derive the correct result. Another thing I learned from this project was that statistical thoughtfulness comes in many shapes and forms. When we were testing the bias and consistency of our estimates in the estimation section, we were practicing statistical thoughtfulness. When we were writing the interpretations of our confidence intervals, credible intervals, and hypothesis tests, we were practicing statistical thoughtfulness. Each project that we had this semester was an exercise in different forms of statistical thoughtfulness, be it an exploration of assumption violation, conveying the connections and meanings of specific terms, further investigating an observation made in class, justifying the creation of prior distributions using given information, or considering which results to present."
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "Project 1: Simulation",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/01-simulation/index.html#code",
    "href": "posts/01-simulation/index.html#code",
    "title": "Project 1: Simulation",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-minimum",
    "href": "posts/01-simulation/index.html#normal-minimum",
    "title": "Project 1: Simulation",
    "section": "Normal Minimum",
    "text": "Normal Minimum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 4   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nnorm_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 4\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 4        # population standard deviation\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\nmins_df &lt;- tibble(mins)\n\nnorm_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstn &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstn\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         -5.92          7.16         2.68           5.29"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-maximum",
    "href": "posts/01-simulation/index.html#normal-maximum",
    "title": "Project 1: Simulation",
    "section": "Normal Maximum",
    "text": "Normal Maximum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 4   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nnorm_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 4\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 4        # population standard deviation\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n\nmaxs_df &lt;- tibble(maxs)\n\nnorm_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximums when n =\", n))\n\nsumstx &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstx\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          26.9          7.10         2.66           14.7"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-minimum",
    "href": "posts/01-simulation/index.html#uniform-minimum",
    "title": "Project 1: Simulation",
    "section": "Uniform Minimum",
    "text": "Uniform Minimum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n, theta1, theta2) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(7, 13, length.out = 1000),\n                  xvals_density = dunif(xvals, 7, 13),\n                  pop = \"uniform(7, 13)\")\n## plot the population model density curve\nunif_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Uniform with Theta1 = 7 and Theta2 = 13\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq)\n\ngenerate_samp_min &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(theta1 = theta1, theta2 = theta2, n = n))\n\nmins_df &lt;- tibble(mins)\n\n\nunif_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstum &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstum\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          7.00         0.723        0.851           8.00"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-maximum",
    "href": "posts/01-simulation/index.html#uniform-maximum",
    "title": "Project 1: Simulation",
    "section": "Uniform Maximum",
    "text": "Uniform Maximum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n, theta1, theta2) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(7, 13, length.out = 1000),\n                  xvals_density = dunif(xvals, 7, 13),\n                  pop = \"uniform(7, 13)\")\n\n## plot the population model density curve\nunif_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Uniform with Theta1 = 7 and Theta2 = 13\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2) / 2   # population mean\nsigma_sq &lt;- (theta2 - theta1)^2 / 12  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\ngenerate_samp_max &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(theta1 = theta1, theta2 = theta2, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\n\nunif_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstux &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstux\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          13.0         0.724        0.851           12.0"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-minimum",
    "href": "posts/01-simulation/index.html#exponential-minimum",
    "title": "Project 1: Simulation",
    "section": "Exponential Minimum",
    "text": "Exponential Minimum\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n## plot the population model density curve\nexp_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with lambda = 0.5\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_min &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(lambda = lambda, n = n))\n\nmins_df &lt;- tibble(mins)\n\nexp_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstem &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstem\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1      0.000154         0.167        0.408          0.408"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-maximum",
    "href": "posts/01-simulation/index.html#exponential-maximum",
    "title": "Project 1: Simulation",
    "section": "Exponential Maximum",
    "text": "Exponential Maximum\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n\n## plot the population model density curve\nexp_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with lambda = 0.5\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\nlambda &lt;- .5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_max &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(lambda = lambda, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\nexp_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstex &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstex\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1          23.2          5.60         2.37           4.57"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-minimum",
    "href": "posts/01-simulation/index.html#beta-minimum",
    "title": "Project 1: Simulation",
    "section": "Beta Minimum",
    "text": "Beta Minimum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n, alpha, beta) |&gt; round(2)\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, 1, length.out = 1000),\n                  xvals_density = dbeta(xvals, 8, 2),\n                  pop = \"beta(8, 2)\")\n\n## plot the population model density curve\nbeta_popmin &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_min, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Beta with alpha = 8 and beta = 2\", subtitle = \"With red line representing the sample minimum\")\n\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n\ngenerate_samp_min &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(alpha = alpha, beta = beta, n = n))\n\nmins_df &lt;- tibble(mins)\n\nbeta_min &lt;- ggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Minimums\",\n       title = paste(\"Sampling Distribution of the \\nSample Minimum when n =\", n))\n\nsumstbm &lt;- mins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins),\n            mean_samp_dist = mean(mins))\nsumstbm\n\n# A tibble: 1 × 4\n  min_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         0.256        0.0110        0.105          0.647"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-maximum",
    "href": "posts/01-simulation/index.html#beta-maximum",
    "title": "Project 1: Simulation",
    "section": "Beta Maximum",
    "text": "Beta Maximum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n, alpha, beta) |&gt; round(2)\n\n# compute the sample min\nsample_max &lt;- max(single_sample)\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, 1, length.out = 1000),\n                  xvals_density = dbeta(xvals, 8, 2),\n                  pop = \"beta(8, 2)\")\n\n## plot the population model density curve\nbeta_popmax &lt;- ggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  geom_vline(xintercept = sample_max, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Beta with alpha = 8 and beta = 2\", subtitle = \"With red line representing the sample maximum\")\n\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta)   # population mean\nsigma_sq &lt;- (alpha*beta) / (((alpha + beta)^2) * (alpha + beta + 1))  # var\nsigma &lt;- sqrt(sigma_sq) # population s.d\n\ngenerate_samp_max &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n\nnsim &lt;- 5000      # number of simulations\n\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(alpha = alpha, beta = beta, n = n))\n\nmaxs_df &lt;- tibble(maxs)\n\nbeta_max &lt;- ggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maximums\",\n       title = paste(\"Sampling Distribution of the \\nSample Maximum when n =\", n))\n\nsumstbx &lt;- maxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs),\n            mean_samp_dist = mean(maxs))\nsumstbx\n\n# A tibble: 1 × 4\n  max_samp_dist var_samp_dist sd_samp_dist mean_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1         0.999       0.00212       0.0460          0.921"
  },
  {
    "objectID": "posts/01-simulation/index.html#methods",
    "href": "posts/01-simulation/index.html#methods",
    "title": "Project 1: Simulation",
    "section": "Methods",
    "text": "Methods\nFor this project, I created two population models for each of the distributions, one with a red line for the sample minimum, and another with a red line for the sample maximum. I then created histograms showing the sample minimums and maximums for each distribution, and included code that calculated the mean, variance, and standard deviation for the table.\nIn order to answer question 2, I found the CDF and pdf of Exp(.2), then put them into the equations needed to find the pdfs for Ymin and Ymax. After that, I used integration to find the expected values and the variances needed to compute the standard error, after which I then compared to the simulated results. I also made rough sketches of the theoretical graphs, which were then compared to the simulated ones."
  },
  {
    "objectID": "posts/01-simulation/index.html#graphs",
    "href": "posts/01-simulation/index.html#graphs",
    "title": "Project 1: Simulation",
    "section": "GRAPHS",
    "text": "GRAPHS"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#normal-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Normal: Population, Sampling Minimum, Sampling Maximum",
    "text": "Normal: Population, Sampling Minimum, Sampling Maximum\n\nnorm_popmin\n\n\n\n\n\n\n\nnorm_popmax\n\n\n\n\n\n\n\nnorm_min\n\n\n\n\n\n\n\nnorm_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#uniform-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Uniform: Population, Sampling Minimum, Sampling Maximum",
    "text": "Uniform: Population, Sampling Minimum, Sampling Maximum\n\nunif_popmin\n\n\n\n\n\n\n\nunif_popmax\n\n\n\n\n\n\n\nunif_min\n\n\n\n\n\n\n\nunif_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#exponential-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Exponential: Population, Sampling Minimum, Sampling Maximum",
    "text": "Exponential: Population, Sampling Minimum, Sampling Maximum\n\nexp_popmin\n\n\n\n\n\n\n\nexp_popmax\n\n\n\n\n\n\n\nexp_min\n\n\n\n\n\n\n\nexp_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-population-sampling-minimum-sampling-maximum",
    "href": "posts/01-simulation/index.html#beta-population-sampling-minimum-sampling-maximum",
    "title": "Project 1: Simulation",
    "section": "Beta: Population, Sampling Minimum, Sampling Maximum",
    "text": "Beta: Population, Sampling Minimum, Sampling Maximum\n\nbeta_popmin\n\n\n\n\n\n\n\nbeta_popmax\n\n\n\n\n\n\n\nbeta_min\n\n\n\n\n\n\n\nbeta_max"
  },
  {
    "objectID": "posts/01-simulation/index.html#table",
    "href": "posts/01-simulation/index.html#table",
    "title": "Project 1: Simulation",
    "section": "TABLE",
    "text": "TABLE\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n5.3\n8.004\n0.4\n0.646\n\n\n\\(\\text{E}(Y_{max})\\)\n14.67\n12\n4.56\n0.922\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n2.67\n0.846\n0.4\n0.106\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.68\n0.829\n2.39\n0.045"
  },
  {
    "objectID": "posts/01-simulation/index.html#questions",
    "href": "posts/01-simulation/index.html#questions",
    "title": "Project 1: Simulation",
    "section": "QUESTIONS",
    "text": "QUESTIONS"
  },
  {
    "objectID": "posts/01-simulation/index.html#question-1",
    "href": "posts/01-simulation/index.html#question-1",
    "title": "Project 1: Simulation",
    "section": "Question 1",
    "text": "Question 1\nBriefly summarise how and compare for each of the above population models. Can you propose a general rule or result for how and compare for a given population?\nFor normal and uniform population models, the standard errors of the sample minimums and maximums are pretty close, with the standard errors for the normal distribution having a difference of only .01 and the standard errors for the uniform distribution having a difference of .017. The beta model’s standard errors are also close with a difference of .061. The exponential model, however, has standard errors that have a large difference. The sample minimum is a small .4 while the sample maximum’s standard error is 2.39, a difference of almost 2. For normal and uniform populations, you can expect the standard errors to be very close together. For beta populations, you can expect the standard errors to be close together but not as close as the normal or uniform populations. For exponential populations, you can expect the standard errors to be far apart. Due to the difference in the exponential population’s standard errors, there can’t be a general rule for all populations. There can be a general rule for three of them: normal, uniform, and beta. That rule would be that the standard errors for the sample minimums and maximums will be close together with a difference of less than one."
  },
  {
    "objectID": "posts/01-simulation/index.html#question-2",
    "href": "posts/01-simulation/index.html#question-2",
    "title": "Project 1: Simulation",
    "section": "Question 2",
    "text": "Question 2\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 5, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * exp(-(2.5) * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\npdf of Ymin: n(1 - F(y))^(n-1) f(y) = 5(1 - e^(-.5y)) ^4 .5e^(-.5y) = 2.5e^(-2.5y)\nE(Ymin) = .4 -&gt; integral of 2.5ye^(-2.5y) (lower bound = 0, upper bound = infinity)\nVar(Ymin) = .32 -&gt; integral of 2.5y^2 * e^(-2.5y) (same bounds as the expected value)\nSE(Ymin) = .32 - .4^2 = .16 = sqrt(.16) = .4\nI find that the theoretical and analytical expected values (.4) and standard errors (.4) are the same. My graph is a bit off, though that is likely because I drew more of a rough sketch, though the simulated graph looks close. It’s likely that with the simulation being generated again, the simulated SE’s and expected values may differ a little bit, but they will still be very close to the theoretical values I found.\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 5, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * exp((2.5) * x) * (1 - exp(-.5))\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\npdf of Ymax: n(F(y))^(n-1) * f(y) =\n5(1-e(-.5x))4 * .5e^(-.5x) =\n2.5e^(-2.5x) * (1 - e(-.5x))4\nE(Ymax) = .467 -&gt; integral of 2.5xe^(-2.5x) * (1-e(-.5x))4 (lower bound of 0, upper bound of infinity)\nVar(Ymax) = 26.71 -&gt; integral of 2.5(x^2) e^(-2.5x) * (1-e(-.5x))4 (lower bound of 0, upper bound of infinity)\nSE(Ymax) = 26.71 - (.467)^2 = 26.5028 = sqrt(26.5028) = 5.148\nThe theoretical expected value is close to the analytical expected value (about .01 off). My theoretical standard error (5.148) was much higher than the analytical standard error (2.39). I think that this shows a high variety in standard errors for Ymax. My graph was different than the simulation, most likely due to my inability to code a correct simulated version of it. However, the x axis seems pretty similar to each other."
  },
  {
    "objectID": "posts/01-simulation/index.html#findings-summary",
    "href": "posts/01-simulation/index.html#findings-summary",
    "title": "Project 1: Simulation",
    "section": "Findings Summary",
    "text": "Findings Summary\nThe sample maximums and minimums of the normal, beta, and uniform distributions have standard errors that are very close, while the standard errors for the exponential distribution’s sample maximums and minimums are very different from each other. As a result, there can be no general rule for all of the distribution’s standard errors, though there can be one for normal, beta, and uniform distributions.\nSimulating the pdfs, expected values, and standard errors of Ymax and Ymin and then comparing them to the theoretical results found that Ymin had very close, if not near identical expected values and standard errors, while Ymax had similar expected values but different standard errors. The difference in standard errors could likely be due to varying varieties of sample maximums. The theoretical graphs were a bit different than the analytical graphs, likely due to coding error, however, the x-axes were pretty similar."
  },
  {
    "objectID": "posts/03-confidence_intervals/index.html",
    "href": "posts/03-confidence_intervals/index.html",
    "title": "Project 3: Confidence Intervals",
    "section": "",
    "text": "library(resample)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/03-confidence_intervals/index.html#findings",
    "href": "posts/03-confidence_intervals/index.html#findings",
    "title": "Project 3: Confidence Intervals",
    "section": "Findings",
    "text": "Findings\nThe large sample assumption for 3 of the 6 settings holds, while 3 of the 6 settings violate the large sample assumption. Both settings with p = .03 and p = .45 are violated when n = 8. When tested using the equations n*p and n*(1-p), the results for p = .03 (.24, 7.76) and p = .45 (3.6, 4.4) are less than 10. This also occurs for the n*p equation when p = .03 and n = 57 (resulting in 1.71), making that the third large sample assumption violation. When n = 793, both equations result in values larger than ten for both population proportions (23.79, 769.21 for when p = .03 and 356.85, 436.15 for when p = .45), meaning that the large sample assumption holds. The same goes for the setting n = 57 and p = .45.\nFor the settings that passed the large sample assumption, the coverage rate was close to or at .9 because the confidence interval is 90%. Due to (n = 57, p = .45) and (n = 793, p = .45) both passing the large sample assumption, it can be observed that the coverage rate doesn’t change much as n increases and is equivalent to .9 when rounded. When n = 8, the coverage rates for p = .03 and p = .45 were .222 and .845 respectfully, while the coverage rate for (n = 57 and p = .03) was .807. These three settings failed the large sample assumption and were all around .85 or less, meaning that when the large sample assumption fails to hold, the coverage rate is not .9 or very close to .9. This means that the intervals given by these settings wouldn’t be a good 90% confidence interval, because the amount of coverage isn’t close enough to .90.\nThe average width for p = .45 decreased as n increased, going from .536 when n = 8 to .0581 when n = 793. This decrease can also be seen for p = .03 as the average width goes from .0882 to .0198. Although the large sample assumption fails when (n = 8, p = .45) it does hold for (n = 57, p = .45) and (n = 793, p = .45), which can explain the average width of (n = 57, p = .45) being larger than (n = 793, p = .45). However, due to (n = 8, p = .03) and (n = 57, p = .03) both also failing the assumption, it is hard to tell if that is truly the case. Since t*(s/sqrt(n)) from the confidence interval formula controls the width of the interval, it makes sense that the width would decrease as n increases due to n being the denominator. It could also potentially explain why (n = 8, p = .45) is still larger than (n = 57, p = .45) and why the first two samples sizes have larger average widths than n = 793 when p = .03 despite the large assumption failing to hold."
  },
  {
    "objectID": "posts/05-hypothesis_testing/index.html",
    "href": "posts/05-hypothesis_testing/index.html",
    "title": "Project 5: Hypothesis Testing",
    "section": "",
    "text": "1.\nI think the author means that people will focus less on p &lt; .05 and the significance of their results and instead focus on actually analyzing and discussing their results. De-emphasizing the p-value and statistical significance would allow for more discussion of less than perfect results. Instead of fixating on results that fit within the constraints of p &lt; .05, researchers could discuss results that may have higher p-values. Doing so would allow for more analysis into what worked and what didn’t and how such results may have occurred, thus improving overall knowledge and potentially even improving the test or experiment used. I also think that getting rid of the use of statistical significance would result in researchers having to consider other means of statistical analysis and inference, forcing them to question the effectiveness and usefulness of different statistical tools, compare models, and analyze their assumptions about the data and results, which is statistical thinking. Instead of basing their explanations primarily around significance, they may analyze their data to see what is actually occurring rather than what is “significant”. I think that to embody statistical thinking, it is required to consider any potential biases and view the problem from a neutral standpoint. For example, it may be a popular sentiment that package theft has been on the rise due to so many neighbors having their packages stolen from their porch. However, if you were to take data on porch piracy and analyze it via graphing and modeling, you would see that porch piracy has actually been decreasing, and that the belief of an increase may come from exposure bias. Statistical thinking isn’t just about acknowledging bias and conducting analyses to disprove common beliefs, however. It’s also about applying statistical tools and techniques to solve problems. For example, politicians and statisticians may work together to decrease the amount of opiate overdoses by using statistics from previous years about emergency response times, education, and the prevalence of narcan use in responding to opiate overdoses. Statisticians could use different tools such as modeling that may result in suggestions of an increase in narcan supply, the necessity of faster emergency response times, and an increase in education. Politicians could then use that information to implement different policies to do such a thing. However, results that may occur from such modeling are not foolproof, and in order to completely embody statistical thinking, it’s necessary to acknowledge the potential shortcomings and limits of models and analyses.\n\n\n2.\nThe author means that treating p-values as a means of determining what is significant and what is insignificant is pointless because this idea of significance comes from a misunderstanding of its original use. Significance is often associated with usefulness, importance, and probability while insignificance is associated with unimportance and improbability. The intention of the use of the word significance was to indicate whether or not more scrutiny was needed - it was never meant to be used in the way that it is used today. Now, p-values are used as a part of determining the rejection of the null hypothesis in hypothesis testing where questions of significance are regularly brought up and the rejection or failure or rejection involves explanations in determining whether or not something was statistically significant. When the author mentions the dichotomization of p-values, they are talking about the categorization of “significant” and “not significant.” The issue with the dichotomization of the p-value is that it incorrectly establishes an authority of the p-value where it can be used to determine the significance or lack thereof in a way where significance is misunderstood as providing proof of plausibility and importance of a result. It contributes to black and white views of results, where one side is deemed worthy of consideration and the other is deemed unworthy and disregarded. This can result in researchers prioritizing certain results when the disregarded results may have provided interesting insight into their research. It creates this idea that significance should be valued above all when there are many other useful statistical inferences that have been disregarded alongside unworthy results.\n\n\n3.\nI agree that p-values shouldn’t have to pass a threshold in order to choose which results to present or highlight because as the paper points out, it could result in bias since the results that fit the proper threshold will get published and reported, leaving out other results that may be of interest. That interest may not be due to “correctness” (i.e. fitting a desired outcome) or what works best, but rather something that can be learned. I think that it’s useful to highlight results that have occurred due to an error or a deviation from the intended experiment or process, as it can help researchers understand what’s happening, how, and what aspect impacts or causes certain results. With that being said, I don’t think that there’s a one size fits all solution to determining what gets presented in publishing, rather it’s a case by case basis. I believe that results that reveal information about the data, regardless of whether they were the desired outcome or the product of a mistake, would serve as noteworthy inclusions given that they help researchers further understand a topic or process. When it comes to “bad” results, it’s always good to look back on what went wrong and how to adjust the study or experiment in the future. I think that reproducibility is an important aspect as well, as it allows others to see where results have come from and further their understanding as to why and what’s been impacted. I think that there are some cases, such as the FDA’s, where p-values can be used to determine what gets published, if only until a better solution for interpreting the results is found since some organizations and researchers have become very reliant on the p-values.\n\n\n4.\nI agree that there’s no one size fits all approach to statistical inference in scientific research because there are too many fields of science and too many types of research to come up with one approach that works well for all of them. Each field has its own standards, with some standards being more similar than others. There is bound to be some conflict in what is appropriate and what is inappropriate in certain fields. I believe that what works best is dependent on the specific study or experiment being done, as some statistical tools and methods might be of better use than others. The FDA example given further in the reading is a good example of this. While it may be possible for them to move away from their reliance on p-values, it’s not an easy task considering that they have been doing it for such a long time and that it’s worked for them. It’s much easier to establish general principles like ATOM and allow the researcher to use what works best for their research, as long as they provide extensive justifications for what they are choosing to do and why.\n\n\n5.\nI think that statistical thoughtfulness means putting everything into consideration. It’s considering all of the prior information and context of the data you are looking at, potential results of the study/experiment, careful designs of your study/experiment, and the utilization of various methods of analysis when looking at results. As said in the text, it’s also about “investing in producing solid data” (page 3, section 3).To demonstrate statistical thoughtfulness, researchers should ask questions pertaining to their data. For example, they should be considering whether or not they made the correct assumptions when analyzing their data, question the precision of their estimates, and whether or not the study was adequately designed. Statistical thoughtfulness to me also means being very thorough in your analysis and study, considering possible outcomes and being able to justify what you’re doing and why. It also involves putting thought into how results and data are conveyed, and their implications. Another way to demonstrate statistical thoughtfulness is by making sure that your data and results are explained in ways that are accessible to your audience. An example of this is seen in 3.2.4, where some authors suggest replacing the words significant and confidence with compatibility so that the terms aren’t misunderstood. Overall, I consider statistical thoughtfulness to be very similar, if not nearly identical to statistical thinking, with statistical thoughtfulness being more thorough and purposeful in what is being done and more research related, while statistical thinking is better suited for non-resarch occurrences and applications.\n\n\n6.\nI think that the authors believe that the problem is that people see words like significance and confidence and make the wrong associations. People associate significance with importance and necessity and confidence with certainty, so when they are looking at confidence intervals, they’re seeing an interval of “correct” data with the incorrect and useless values left out of the interval and to be considered no longer. In combination with the prioritization of a small p-value due to a misconstrued understanding of significance, the author believes that there’s a risk of overconfidence which I believe may stem from the acceptance of confidence intervals that are too narrow simply because it contains a certain value. I think the overconfidence is not necessarily about the narrowness of the interval, but also in regards to misunderstood interpretations of confidence intervals, where the intervals are treated less as estimates and more as significant values. The authors introduced the term compatibility to try and rid p-values and confidence intervals of misunderstood associations and to encourage people to view confidence intervals as producing estimates that are compatible with the data under the created model instead of values that the data should fall into. It’s a similar situation with p-values, where compatibility can be used to shape the view of p-values as “measuring compatibility between hypothesis and data” (section 3.2.4) instead of p-values as determining significance. I think that the overall goal is to view confidence intervals and p-values in relation to the data instead of as a descriptor of the data.\nI agree that there may be an issue with people misunderstanding the words significance and confidence, resulting in the misunderstanding of confidence intervals and p-values. However, I don’t really agree that the solution is to change the wording because I feel like compatibility may still be misunderstood in a similar way to the words significance and confidence, thus resulting in the similar treatment. I think the better course of action is to provide more in depth explanations about what is actually happening and what the p-values and confidence intervals are actually telling you and what they actually represent. Changing the words to compatibility may help with some context, but I think in order to get people to fully understand what is going on, you need to explain it to them instead of relying on implicit understanding.\n\n\n7.\nAt the end of section four, the quote “no publication policy will be perfect. Science is inherently challenging and we must always be willing to accept that a certain proportion of research is potentially false” really stuck out to me because while it seems obvious that some research is wrong, I never really consider that when I read research papers. If a paper is published then that means it’s most likely been peer-reviewed and checked by experts in the field, so the possibility of research being false always seems so low to me. This quote has made me realize that I view research in a very idealistic way without even realizing it. I tend to trust papers that have been published due to their potential peer-reviewed status and while the idea that it’s incorrect may be a thought in the back of my mind, I often don’t entertain it, choosing to trust the researcher. Unless the author of the paper themselves acknowledges their research’s shortcomings or I come across a different paper that criticizes it, I don’t look at papers as critically as I should be, especially if it’s from a field that I have little knowledge about. This quote has revealed a blind spot that I have in regards to research and publications and has made me realize that I need to be more outwardly critical of what I read instead of assuming that the author is right."
  }
]